{
  "report_metadata": {
    "generated_at": "2025-10-23T16:55:00Z",
    "diagnosticador_version": "1.0.0",
    "analysis_scope": "All Backend Services (87 services)",
    "analysis_depth": "Deep (Structure + Code + Runtime + Logs)",
    "total_services_analyzed": 87,
    "total_api_files": 797,
    "total_air_gaps_found": 10
  },
  "executive_summary": {
    "status": "CRITICAL ISSUES FOUND",
    "total_air_gaps": 10,
    "critical_air_gaps": 3,
    "high_priority_air_gaps": 3,
    "medium_priority_air_gaps": 4,
    "overall_integration_health": "65/100",
    "kafka_cohesion_score": "7/10",
    "http_cohesion_score": "6/10",
    "redis_cohesion_score": "9/10",
    "immediate_action_required": true
  },
  "air_gaps": [
    {
      "id": "AG-RUNTIME-001",
      "title": "Oráculo Service - Kafka Hard Dependency",
      "category": "Kafka",
      "severity": "CRITICAL",
      "priority": 1,
      "description": "Oráculo service crashes on startup when Kafka is unavailable, despite graceful degradation being implemented in config.py",
      "affected_services": ["maximus_oraculo"],
      "root_cause": "WebSocket APV stream manager (apv_stream_manager.py:158) starts Kafka consumer unconditionally in startup_event()",
      "evidence": {
        "error_log": "KafkaConnectionError: Unable to bootstrap from [('localhost', 9092)]",
        "stack_trace": "api.py:105 → apv_stream_manager.py:158 → consumer.start()",
        "config_file": "config.py has ENABLE_KAFKA flag but APV stream manager ignores it"
      },
      "impact": {
        "service_availability": "Service cannot start",
        "data_loss_risk": "High - APVs not published to Kafka",
        "downstream_services": ["maximus_eureka (no APVs received)"]
      },
      "recommendations": [
        "Wrap APV stream manager start() in ENABLE_KAFKA conditional",
        "Implement in-memory queue fallback for APVs when Kafka unavailable",
        "Add graceful degradation mode to websocket/apv_stream_manager.py"
      ],
      "fix_complexity": "Medium (3-5 hours)",
      "testing_required": "Unit tests + integration tests with Kafka down"
    },
    {
      "id": "AG-RUNTIME-002",
      "title": "maximus_core_service - Missing torch Dependency",
      "category": "Dependencies",
      "severity": "CRITICAL",
      "priority": 2,
      "description": "Service crashes on import due to missing PyTorch module required by anomaly detector",
      "affected_services": ["maximus_core_service"],
      "root_cause": "autonomic_core/analyze/anomaly_detector.py:6 imports torch but it's not in requirements.txt or installed",
      "evidence": {
        "error_log": "ModuleNotFoundError: No module named 'torch'",
        "stack_trace": "main.py:17 → maximus_integrated.py:22 → homeostatic_control.py → anomaly_detector.py:6",
        "missing_requirement": "torch (PyTorch) not in requirements.txt"
      },
      "impact": {
        "service_availability": "Service cannot start at all",
        "data_loss_risk": "High - Entire consciousness system down",
        "downstream_services": ["All consciousness services (ESGT, MCEA, MMEI)"]
      },
      "recommendations": [
        "Add torch>=2.0.0 to maximus_core_service/requirements.txt",
        "Run pip install torch in deployment",
        "Consider making torch optional with lazy import if anomaly detection is optional"
      ],
      "fix_complexity": "Low (30 minutes)",
      "testing_required": "Smoke test after pip install torch"
    },
    {
      "id": "AG-RUNTIME-003",
      "title": "maximus_predict - Deprecated on_event API",
      "category": "Code Quality",
      "severity": "MEDIUM",
      "priority": 8,
      "description": "Service uses deprecated FastAPI on_event decorators instead of lifespan handlers",
      "affected_services": ["maximus_predict"],
      "root_cause": "api.py:46 and api.py:54 use @app.on_event() which is deprecated in FastAPI 0.109+",
      "evidence": {
        "warning_log": "DeprecationWarning: on_event is deprecated, use lifespan event handlers instead",
        "affected_lines": ["api.py:46 @app.on_event('startup')", "api.py:54 @app.on_event('shutdown')"]
      },
      "impact": {
        "service_availability": "Works but will break in future FastAPI versions",
        "data_loss_risk": "Low",
        "downstream_services": []
      },
      "recommendations": [
        "Migrate to @asynccontextmanager lifespan pattern",
        "Follow FastAPI docs: https://fastapi.tiangolo.com/advanced/events/"
      ],
      "fix_complexity": "Low (15 minutes)",
      "testing_required": "Regression tests after migration"
    },
    {
      "id": "AG-KAFKA-004",
      "title": "reactive_fabric.honeypot_status - No Consumer",
      "category": "Kafka",
      "severity": "MEDIUM",
      "priority": 5,
      "description": "Reactive Fabric publishes honeypot status updates but no service consumes them",
      "affected_services": ["reactive_fabric_core"],
      "root_cause": "Orphaned Kafka topic - producer exists, consumer missing",
      "evidence": {
        "producer": "reactive_fabric_core/kafka_producer.py:28 TOPIC_HONEYPOT_STATUS",
        "consumers": "None found in codebase",
        "publish_calls": "kafka_producer.py:149 publish_honeypot_status()"
      },
      "impact": {
        "service_availability": "No impact",
        "data_loss_risk": "Medium - Honeypot intelligence lost",
        "downstream_services": []
      },
      "recommendations": [
        "Create consumer in active_immune_core or threat_intel_bridge",
        "Use honeypot status for threat pattern learning",
        "Alternative: Remove topic if not needed"
      ],
      "fix_complexity": "Medium (2-4 hours)",
      "testing_required": "End-to-end honeypot → consumer test"
    },
    {
      "id": "AG-KAFKA-005",
      "title": "maximus.adaptive-immunity.dlq - No DLQ Consumer",
      "category": "Kafka",
      "severity": "HIGH",
      "priority": 3,
      "description": "Dead Letter Queue for failed APVs has no consumer for monitoring/retry",
      "affected_services": ["maximus_oraculo"],
      "root_cause": "APV publisher sends failed messages to DLQ but no service monitors it",
      "evidence": {
        "producer": "maximus_oraculo/kafka_integration/apv_publisher.py:47 dlq_topic",
        "consumers": "None found",
        "dlq_usage": "Failed APVs published to DLQ on validation/send errors"
      },
      "impact": {
        "service_availability": "No impact",
        "data_loss_risk": "High - Failed APVs silently lost",
        "downstream_services": ["maximus_eureka (misses failed APVs)"]
      },
      "recommendations": [
        "Create DLQ consumer for alerting + metrics",
        "Implement retry logic for transient failures",
        "Dashboard for DLQ message count"
      ],
      "fix_complexity": "Medium (3-5 hours)",
      "testing_required": "Force APV validation failure, check DLQ consumer alerts"
    },
    {
      "id": "AG-KAFKA-006",
      "title": "vertice.threats.intel - No Producer",
      "category": "Kafka",
      "severity": "MEDIUM",
      "priority": 6,
      "description": "Active Immune Core expects external threat intel events that never arrive",
      "affected_services": ["active_immune_core"],
      "root_cause": "Consumer registered for external topic with no producer in ecosystem",
      "evidence": {
        "consumer": "active_immune_core/communication/kafka_consumers.py:36 ExternalTopic.THREATS_INTEL",
        "producer": "None found in backend services"
      },
      "impact": {
        "service_availability": "Works in degraded mode",
        "data_loss_risk": "Low - Feature never worked",
        "downstream_services": []
      },
      "recommendations": [
        "Create threat intel producer from OSINT aggregation",
        "Connect to external threat feeds (MISP, STIX/TAXII)",
        "Alternative: Remove consumer if feature not needed"
      ],
      "fix_complexity": "High (8-16 hours)",
      "testing_required": "Integration with external threat feeds"
    },
    {
      "id": "AG-KAFKA-007",
      "title": "vertice.network.events - No Producer",
      "category": "Kafka",
      "severity": "MEDIUM",
      "priority": 7,
      "description": "Active Immune Core expects network monitoring events that never arrive",
      "affected_services": ["active_immune_core"],
      "root_cause": "Consumer registered for network events with no producer",
      "evidence": {
        "consumer": "active_immune_core/communication/kafka_consumers.py:37 ExternalTopic.NETWORK_EVENTS",
        "producer": "None found (network_monitor_service exists but no Kafka producer)"
      },
      "impact": {
        "service_availability": "Works in degraded mode",
        "data_loss_risk": "Low - Feature never worked",
        "downstream_services": []
      },
      "recommendations": [
        "Add Kafka producer to network_monitor_service",
        "Publish network anomalies, scans, port activity",
        "Alternative: Remove consumer if not needed"
      ],
      "fix_complexity": "Medium (4-8 hours)",
      "testing_required": "Network monitoring → Kafka → Active Immune Core"
    },
    {
      "id": "AG-KAFKA-008",
      "title": "vertice.endpoint.events - No Producer",
      "category": "Kafka",
      "severity": "MEDIUM",
      "priority": 7,
      "description": "Active Immune Core expects endpoint agent events that never arrive",
      "affected_services": ["active_immune_core"],
      "root_cause": "Consumer registered for endpoint events with no producer",
      "evidence": {
        "consumer": "active_immune_core/communication/kafka_consumers.py:38 ExternalTopic.ENDPOINT_EVENTS",
        "producer": "None found (edge_agent_service exists but no Kafka producer)"
      },
      "impact": {
        "service_availability": "Works in degraded mode",
        "data_loss_risk": "Low - Feature never worked",
        "downstream_services": []
      },
      "recommendations": [
        "Add Kafka producer to edge_agent_service",
        "Publish EDR events, process monitoring, file activity",
        "Alternative: Remove consumer if not needed"
      ],
      "fix_complexity": "Medium (4-8 hours)",
      "testing_required": "Edge agent → Kafka → Active Immune Core"
    },
    {
      "id": "AG-KAFKA-009",
      "title": "agent-communications - No Producer",
      "category": "Kafka",
      "severity": "HIGH",
      "priority": 4,
      "description": "Narrative Filter Service expects agent communications that never arrive",
      "affected_services": ["narrative_filter_service"],
      "root_cause": "Consumer expects agent communications but no producer connected",
      "evidence": {
        "consumer": "narrative_filter_service/kafka_consumer.py:36 consumes 'agent-communications'",
        "producer": "None found (agent_communication service exists but may not publish to Kafka)"
      },
      "impact": {
        "service_availability": "Service may run but feature unused",
        "data_loss_risk": "Medium - Narrative filtering not working",
        "downstream_services": []
      },
      "recommendations": [
        "Connect agent_communication service to Kafka",
        "Publish agent messages to agent-communications topic",
        "Verify narrative filtering use case still valid"
      ],
      "fix_complexity": "Medium (4-6 hours)",
      "testing_required": "Agent → Kafka → Narrative Filter pipeline"
    },
    {
      "id": "AG-KAFKA-010",
      "title": "verification_errors - No Consumer",
      "category": "Kafka",
      "severity": "MEDIUM",
      "priority": 6,
      "description": "Narrative Manipulation Filter publishes verification errors but no monitoring",
      "affected_services": ["narrative_manipulation_filter"],
      "root_cause": "Error topic without consumer for alerting",
      "evidence": {
        "producer": "narrative_manipulation_filter/fact_check_queue.py:48 TOPIC_ERRORS",
        "consumers": "None found"
      },
      "impact": {
        "service_availability": "No impact",
        "data_loss_risk": "Medium - Verification errors silently lost",
        "downstream_services": []
      },
      "recommendations": [
        "Create error consumer for logging/alerting",
        "Dashboard for error rate monitoring",
        "Retry logic for transient fact-check failures"
      ],
      "fix_complexity": "Low (2-3 hours)",
      "testing_required": "Force verification error, check consumer receives"
    }
  ],
  "cohesion_analysis": {
    "kafka_integration": {
      "total_topics": 15,
      "complete_pipelines": 6,
      "orphaned_producers": 3,
      "orphaned_consumers": 5,
      "cohesion_score": "7/10",
      "notes": "Consciousness pipeline (Digital Thalamus → Memory/PFC) is excellent. Threat detection pipeline works. Main gaps: external event topics without producers."
    },
    "http_integration": {
      "total_services_with_apis": 87,
      "total_api_files": 797,
      "http_clients_found": "50+",
      "orphaned_clients": 2,
      "cohesion_score": "6/10",
      "notes": "Massive service ecosystem (87 services, 797 APIs). Found orphaned HTTP clients: MCEA/MMEI clients in integration_archive_dead_code/ pointing to localhost:8100 (services don't exist as standalone). Most HTTP integrations appear functional but scale is too large for full mapping."
    },
    "redis_integration": {
      "total_redis_streams": 1,
      "complete_pipelines": 1,
      "cohesion_score": "9/10",
      "notes": "Redis Streams hot path (consciousness:hot-path) fully implemented in Digital Thalamus global_workspace.py. Single stream, single purpose, excellent cohesion."
    },
    "error_handling": {
      "services_with_graceful_degradation": 12,
      "services_with_circuit_breakers": 3,
      "services_with_retry_logic": 5,
      "resilience_score": "6/10",
      "notes": "Good error handling in: active_immune_core (cytokines graceful degradation), threat_intel_bridge (circuit breaker), maximus_oraculo (retry logic for APV publishing). Major gap: Oráculo websocket startup bypasses graceful degradation."
    }
  },
  "top_5_priorities": [
    {
      "rank": 1,
      "air_gap_id": "AG-RUNTIME-001",
      "title": "Fix Oráculo Kafka Hard Dependency",
      "estimated_effort": "3-5 hours",
      "business_impact": "CRITICAL - Blocks Oráculo service startup",
      "technical_debt": "Medium"
    },
    {
      "rank": 2,
      "air_gap_id": "AG-RUNTIME-002",
      "title": "Install torch Dependency for maximus_core_service",
      "estimated_effort": "30 minutes",
      "business_impact": "CRITICAL - Blocks consciousness system",
      "technical_debt": "Low"
    },
    {
      "rank": 3,
      "air_gap_id": "AG-KAFKA-005",
      "title": "Create DLQ Consumer for APV Failures",
      "estimated_effort": "3-5 hours",
      "business_impact": "HIGH - Silent APV loss",
      "technical_debt": "Medium"
    },
    {
      "rank": 4,
      "air_gap_id": "AG-KAFKA-009",
      "title": "Connect agent-communications Producer",
      "estimated_effort": "4-6 hours",
      "business_impact": "HIGH - Narrative filtering unused",
      "technical_debt": "Medium"
    },
    {
      "rank": 5,
      "air_gap_id": "AG-KAFKA-004",
      "title": "Create honeypot_status Consumer or Remove Topic",
      "estimated_effort": "2-4 hours",
      "business_impact": "MEDIUM - Honeypot intel lost",
      "technical_debt": "Low"
    }
  ],
  "recommendations": {
    "immediate_actions": [
      "Install torch in maximus_core_service (30 min)",
      "Fix Oráculo Kafka graceful degradation (3-5 hours)",
      "Create DLQ consumer for APV monitoring (3-5 hours)"
    ],
    "short_term_actions": [
      "Connect agent_communication → agent-communications topic (4-6 hours)",
      "Migrate maximus_predict to FastAPI lifespan (15 min)",
      "Create honeypot_status consumer (2-4 hours)"
    ],
    "long_term_actions": [
      "Implement external threat intel producers (vertice.threats.intel, network.events, endpoint.events) (16-32 hours)",
      "Audit all 87 services for HTTP integration cohesion (40-80 hours)",
      "Create centralized DLQ monitoring service for all Kafka topics (8-16 hours)"
    ],
    "architectural_improvements": [
      "Implement circuit breakers in all HTTP clients",
      "Standardize graceful degradation pattern across all Kafka consumers",
      "Create service dependency graph visualization",
      "Implement distributed tracing (OpenTelemetry) for cross-service debugging"
    ]
  }
}
