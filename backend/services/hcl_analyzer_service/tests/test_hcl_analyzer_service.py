"""Unit tests for HCL Analyzer Service.

DOUTRINA VÃ‰RTICE - ARTIGO II: PAGANI Standard
Tests cover actual production implementation:
- Health check endpoint
- Metrics analysis with anomaly detection
- CPU spike detection (statistical: mean + 2*std)
- Memory critical threshold (>90%)
- Error rate elevation (>5%)
- Analysis history tracking
- Model validation (AnomalyType, Anomaly, AnalysisResult, SystemMetrics)
- Edge cases and boundary conditions
"""

# Import the FastAPI app
import sys
from datetime import datetime

import pytest
import pytest_asyncio
from httpx import AsyncClient

sys.path.insert(0, "/home/juan/vertice-dev/backend/services/hcl_analyzer_service")
from main import app, historical_metrics
from models import AnalysisResult, Anomaly, AnomalyType, SystemMetrics

# ==================== FIXTURES ====================


@pytest_asyncio.fixture
async def client():
    """Create async HTTP client for testing FastAPI app."""
    async with AsyncClient(app=app, base_url="http://test") as ac:
        # Clear historical metrics before each test
        historical_metrics.clear()
        yield ac


def create_metrics(cpu=50.0, memory=50.0, error_rate=0.01):
    """Helper to create SystemMetrics payload."""
    return {
        "timestamp": datetime.now().isoformat(),
        "cpu_usage": cpu,
        "memory_usage": memory,
        "disk_io_rate": 1000.0,
        "network_io_rate": 2000.0,
        "avg_latency_ms": 50.0,
        "error_rate": error_rate,
        "service_status": {"api": "healthy", "db": "healthy"},
    }


# ==================== HEALTH CHECK TESTS ====================


@pytest.mark.asyncio
class TestHealthEndpoint:
    """Test health check endpoint."""

    async def test_health_check_returns_healthy_status(self, client):
        """Test health endpoint returns operational status."""
        response = await client.get("/health")

        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"
        assert "operational" in data["message"].lower()


# ==================== METRICS ANALYSIS TESTS ====================


@pytest.mark.asyncio
class TestAnalyzeMetricsEndpoint:
    """Test metrics analysis endpoint."""

    async def test_analyze_healthy_system(self, client):
        """Test analysis of healthy system with no anomalies."""
        payload = {"current_metrics": create_metrics(cpu=50.0, memory=50.0, error_rate=0.01)}

        response = await client.post("/analyze_metrics", json=payload)

        assert response.status_code == 200
        data = response.json()
        assert "timestamp" in data
        assert data["overall_health_score"] == 1.0
        assert len(data["anomalies"]) == 0
        assert data["requires_intervention"] is False
        assert len(data["recommendations"]) == 0

    async def test_analyze_detects_cpu_spike(self, client):
        """Test detection of CPU spike anomaly (mean + 2*std)."""
        # Seed historical data with stable CPU ~50%
        for _ in range(10):
            payload = {"current_metrics": create_metrics(cpu=50.0)}
            await client.post("/analyze_metrics", json=payload)

        # Now send spike
        payload = {"current_metrics": create_metrics(cpu=95.0)}
        response = await client.post("/analyze_metrics", json=payload)

        assert response.status_code == 200
        data = response.json()
        assert len(data["anomalies"]) > 0

        # Check CPU spike anomaly
        cpu_anomaly = next((a for a in data["anomalies"] if a["metric_name"] == "cpu_usage"), None)
        assert cpu_anomaly is not None
        assert cpu_anomaly["type"] == "spike"
        assert cpu_anomaly["severity"] == 0.8
        assert cpu_anomaly["current_value"] == 95.0

        assert data["requires_intervention"] is True
        assert data["overall_health_score"] < 1.0
        assert any("cpu" in r.lower() for r in data["recommendations"])

    async def test_analyze_detects_critical_memory(self, client):
        """Test detection of critical memory usage (>90%)."""
        payload = {"current_metrics": create_metrics(memory=95.0)}
        response = await client.post("/analyze_metrics", json=payload)

        assert response.status_code == 200
        data = response.json()
        assert len(data["anomalies"]) > 0

        # Check memory anomaly
        mem_anomaly = next((a for a in data["anomalies"] if a["metric_name"] == "memory_usage"), None)
        assert mem_anomaly is not None
        assert mem_anomaly["type"] == "outlier"
        assert mem_anomaly["severity"] == 0.9
        assert mem_anomaly["current_value"] == 95.0

        assert data["requires_intervention"] is True
        assert data["overall_health_score"] < 1.0
        assert any("memory" in r.lower() for r in data["recommendations"])

    async def test_analyze_detects_elevated_error_rate(self, client):
        """Test detection of elevated error rate (>5%)."""
        payload = {"current_metrics": create_metrics(error_rate=0.08)}  # 8% error rate
        response = await client.post("/analyze_metrics", json=payload)

        assert response.status_code == 200
        data = response.json()
        assert len(data["anomalies"]) > 0

        # Check error rate anomaly
        error_anomaly = next((a for a in data["anomalies"] if a["metric_name"] == "error_rate"), None)
        assert error_anomaly is not None
        assert error_anomaly["type"] == "trend"
        assert error_anomaly["severity"] == 0.7
        assert error_anomaly["current_value"] == 0.08

        assert data["requires_intervention"] is True
        assert data["overall_health_score"] < 1.0
        assert any("error" in r.lower() or "log" in r.lower() for r in data["recommendations"])

    async def test_analyze_multiple_anomalies(self, client):
        """Test detection of multiple simultaneous anomalies."""
        payload = {
            "current_metrics": create_metrics(
                cpu=50.0,
                memory=95.0,  # Critical
                error_rate=0.10,  # High
            )
        }
        response = await client.post("/analyze_metrics", json=payload)

        assert response.status_code == 200
        data = response.json()
        assert len(data["anomalies"]) == 2  # Memory + Error rate
        assert data["requires_intervention"] is True
        # Health score should be reduced significantly
        assert data["overall_health_score"] < 0.6

    async def test_analyze_health_score_calculation(self, client):
        """Test that health score is calculated correctly."""
        # Healthy system
        payload = {"current_metrics": create_metrics(cpu=50.0, memory=50.0, error_rate=0.01)}
        response = await client.post("/analyze_metrics", json=payload)
        healthy_score = response.json()["overall_health_score"]
        assert healthy_score == 1.0

        # System with memory issue
        historical_metrics.clear()
        payload = {"current_metrics": create_metrics(memory=95.0)}
        response = await client.post("/analyze_metrics", json=payload)
        memory_issue_score = response.json()["overall_health_score"]
        # Health score reduced by 0.3 for memory
        assert memory_issue_score == 0.7

        # System with error rate issue
        historical_metrics.clear()
        payload = {"current_metrics": create_metrics(error_rate=0.10)}
        response = await client.post("/analyze_metrics", json=payload)
        error_issue_score = response.json()["overall_health_score"]
        # Health score reduced by 0.15 for error rate
        assert error_issue_score == 0.85

    async def test_analyze_health_score_clamped_to_zero(self, client):
        """Test that health score cannot go below 0."""
        # All possible anomalies
        payload = {"current_metrics": create_metrics(cpu=50.0, memory=95.0, error_rate=0.10)}
        response = await client.post("/analyze_metrics", json=payload)

        data = response.json()
        assert data["overall_health_score"] >= 0.0


# ==================== HISTORICAL METRICS TESTS ====================


@pytest.mark.asyncio
class TestHistoricalMetrics:
    """Test historical metrics tracking."""

    async def test_historical_metrics_stored(self, client):
        """Test that analyzed metrics are stored in history."""
        # Initial state
        assert len(historical_metrics) == 0

        # Analyze metrics
        payload = {"current_metrics": create_metrics()}
        await client.post("/analyze_metrics", json=payload)

        # Check stored
        assert len(historical_metrics) == 1
        assert historical_metrics[0].cpu_usage == 50.0

    async def test_historical_metrics_rolling_window(self, client):
        """Test that historical metrics maintain rolling window of 100."""
        # Send 105 metrics
        for i in range(105):
            payload = {"current_metrics": create_metrics(cpu=float(i))}
            await client.post("/analyze_metrics", json=payload)

        # Should keep only last 100
        assert len(historical_metrics) == 100
        # First entry should be #5 (0-4 were dropped)
        assert historical_metrics[0].cpu_usage == 5.0
        # Last entry should be #104
        assert historical_metrics[-1].cpu_usage == 104.0

    async def test_cpu_spike_detection_requires_history(self, client):
        """Test that CPU spike detection needs >5 historical data points."""
        # Send only 3 metrics (not enough for statistical analysis)
        for i in range(3):
            payload = {"current_metrics": create_metrics(cpu=50.0)}
            await client.post("/analyze_metrics", json=payload)

        # Send spike - should NOT be detected (insufficient history)
        payload = {"current_metrics": create_metrics(cpu=95.0)}
        response = await client.post("/analyze_metrics", json=payload)

        data = response.json()
        # No CPU spike should be detected (len(cpu_usages) <= 5)
        cpu_anomaly = next((a for a in data["anomalies"] if a["metric_name"] == "cpu_usage"), None)
        assert cpu_anomaly is None


# ==================== ANALYSIS HISTORY ENDPOINT TESTS ====================


@pytest.mark.asyncio
class TestAnalysisHistoryEndpoint:
    """Test analysis history retrieval endpoint."""

    async def test_analysis_history_empty(self, client):
        """Test retrieving empty analysis history."""
        response = await client.get("/analysis_history")

        assert response.status_code == 200
        data = response.json()
        assert isinstance(data, list)
        assert len(data) == 0

    async def test_analysis_history_with_limit(self, client):
        """Test retrieving analysis history with limit parameter."""
        response = await client.get("/analysis_history?limit=5")

        assert response.status_code == 200
        data = response.json()
        assert isinstance(data, list)
        # Currently mocked to return empty list
        assert len(data) == 0


# ==================== MODEL VALIDATION TESTS ====================


class TestModels:
    """Test Pydantic models."""

    def test_anomaly_type_enum_values(self):
        """Test AnomalyType enum has expected values."""
        assert AnomalyType.SPIKE.value == "spike"
        assert AnomalyType.DROP.value == "drop"
        assert AnomalyType.TREND.value == "trend"
        assert AnomalyType.OUTLIER.value == "outlier"

    def test_anomaly_creation(self):
        """Test creating Anomaly model."""
        anomaly = Anomaly(
            type=AnomalyType.SPIKE,
            metric_name="cpu_usage",
            current_value=95.0,
            severity=0.8,
            description="CPU spike detected",
        )

        assert anomaly.type == AnomalyType.SPIKE
        assert anomaly.metric_name == "cpu_usage"
        assert anomaly.current_value == 95.0
        assert anomaly.severity == 0.8
        assert "spike" in anomaly.description.lower()

    def test_system_metrics_creation(self):
        """Test creating SystemMetrics model."""
        metrics = SystemMetrics(
            timestamp="2025-10-07T12:00:00",
            cpu_usage=50.0,
            memory_usage=60.0,
            disk_io_rate=1000.0,
            network_io_rate=2000.0,
            avg_latency_ms=50.0,
            error_rate=0.01,
            service_status={"api": "healthy", "db": "healthy"},
        )

        assert metrics.cpu_usage == 50.0
        assert metrics.memory_usage == 60.0
        assert metrics.service_status["api"] == "healthy"

    def test_analysis_result_creation(self):
        """Test creating AnalysisResult model."""
        result = AnalysisResult(
            timestamp="2025-10-07T12:00:00",
            overall_health_score=0.85,
            anomalies=[],
            trends={"cpu_trend": "stable"},
            recommendations=["Monitor CPU usage"],
            requires_intervention=False,
        )

        assert result.overall_health_score == 0.85
        assert len(result.anomalies) == 0
        assert result.trends["cpu_trend"] == "stable"
        assert len(result.recommendations) == 1
        assert result.requires_intervention is False

    def test_analysis_result_default_timestamp(self):
        """Test AnalysisResult generates default timestamp."""
        result = AnalysisResult(
            overall_health_score=1.0, anomalies=[], trends={}, recommendations=[], requires_intervention=False
        )

        # Should have generated timestamp
        assert result.timestamp is not None
        datetime.fromisoformat(result.timestamp)


# ==================== EDGE CASES ====================


@pytest.mark.asyncio
class TestEdgeCases:
    """Test edge cases and boundary conditions."""

    async def test_analyze_with_zero_values(self, client):
        """Test analysis with zero values."""
        payload = {"current_metrics": create_metrics(cpu=0.0, memory=0.0, error_rate=0.0)}

        response = await client.post("/analyze_metrics", json=payload)

        assert response.status_code == 200
        data = response.json()
        # Should succeed without errors
        assert "overall_health_score" in data

    async def test_analyze_with_boundary_values(self, client):
        """Test analysis with boundary values (100%, 0%)."""
        payload = {"current_metrics": create_metrics(cpu=100.0, memory=100.0, error_rate=1.0)}

        response = await client.post("/analyze_metrics", json=payload)

        assert response.status_code == 200
        data = response.json()
        # Should detect anomalies at boundary
        assert len(data["anomalies"]) > 0

    async def test_analyze_with_memory_exactly_90_percent(self, client):
        """Test memory threshold at exactly 90% (boundary)."""
        payload = {"current_metrics": create_metrics(memory=90.0)}
        response = await client.post("/analyze_metrics", json=payload)

        data = response.json()
        # Should NOT trigger (condition is >90)
        mem_anomaly = next((a for a in data["anomalies"] if a["metric_name"] == "memory_usage"), None)
        assert mem_anomaly is None

    async def test_analyze_with_memory_just_above_90_percent(self, client):
        """Test memory threshold just above 90%."""
        payload = {"current_metrics": create_metrics(memory=90.1)}
        response = await client.post("/analyze_metrics", json=payload)

        data = response.json()
        # Should trigger
        mem_anomaly = next((a for a in data["anomalies"] if a["metric_name"] == "memory_usage"), None)
        assert mem_anomaly is not None

    async def test_analyze_with_error_rate_exactly_5_percent(self, client):
        """Test error rate threshold at exactly 5% (boundary)."""
        payload = {"current_metrics": create_metrics(error_rate=0.05)}
        response = await client.post("/analyze_metrics", json=payload)

        data = response.json()
        # Should NOT trigger (condition is >0.05)
        error_anomaly = next((a for a in data["anomalies"] if a["metric_name"] == "error_rate"), None)
        assert error_anomaly is None

    async def test_analyze_with_error_rate_just_above_5_percent(self, client):
        """Test error rate threshold just above 5%."""
        payload = {"current_metrics": create_metrics(error_rate=0.051)}
        response = await client.post("/analyze_metrics", json=payload)

        data = response.json()
        # Should trigger
        error_anomaly = next((a for a in data["anomalies"] if a["metric_name"] == "error_rate"), None)
        assert error_anomaly is not None

    async def test_analyze_invalid_metrics_returns_422(self, client):
        """Test analysis with invalid metrics structure."""
        payload = {
            "current_metrics": {
                # Missing required fields
                "cpu_usage": 50.0
            }
        }

        response = await client.post("/analyze_metrics", json=payload)

        # Pydantic validation should fail
        assert response.status_code == 422

    async def test_analyze_with_negative_values_returns_422(self, client):
        """Test analysis with negative values (invalid)."""
        payload = {"current_metrics": create_metrics(cpu=-10.0)}

        response = await client.post("/analyze_metrics", json=payload)

        # Should succeed (no validation for negative in model)
        # But in production, this would be validated
        assert response.status_code in [200, 422]
