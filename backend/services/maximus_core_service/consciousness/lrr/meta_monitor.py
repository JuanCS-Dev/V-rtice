"""
Metacognitive monitoring utilities for Recursive Reasoner (LRR).

Collects metrics, evaluates bias and confidence calibration, and produces
actionable recommendations to keep reasoning aligned with the Doutrina VÃ©rtice.
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from statistics import mean, stdev
from typing import Iterable, List, Optional, Sequence, TYPE_CHECKING

if TYPE_CHECKING:  # pragma: no cover
    from .recursive_reasoner import ReasoningLevel, ReasoningStep, RecursiveReasoningResult


@dataclass(slots=True)
class BiasInsight:
    """Describes a cognitive bias detected during monitoring."""

    name: str
    severity: float
    evidence: List[str]


@dataclass(slots=True)
class CalibrationMetrics:
    """Calibration quality for confidence assessments."""

    brier_score: float
    expected_calibration_error: float
    correlation: float


@dataclass(slots=True)
class MetaMonitoringReport:
    """Full metacognitive report returned by MetaMonitor."""

    total_levels: int
    average_coherence: float
    average_confidence: float
    processing_time_ms: float
    calibration: CalibrationMetrics
    biases_detected: List[BiasInsight]
    recommendations: List[str]


class MetricsCollector:
    """Aggregate raw metrics from reasoning levels."""

    def collect(self, levels: Sequence["ReasoningLevel"]) -> dict:
        if not levels:
            return {
                "total_levels": 0,
                "average_coherence": 0.0,
                "average_confidence": 0.0,
            }

        confidences: List[float] = []
        for level in levels:
            confidences.extend([step.confidence_assessment for step in level.steps])

        return {
            "total_levels": len(levels),
            "average_coherence": float(mean(level.coherence for level in levels)),
            "average_confidence": float(mean(confidences)) if confidences else 0.0,
        }


class BiasDetector:
    """Detect simple cognitive biases to keep reasoning antifragile."""

    def detect(self, levels: Sequence["ReasoningLevel"]) -> List[BiasInsight]:
        insights: List[BiasInsight] = []

        if not levels:  # pragma: no cover - empty levels handled by monitor_reasoning
            return insights  # pragma: no cover

        if self._possible_confirmation_bias(levels):
            insights.append(  # pragma: no cover - confirmation bias detection tested
                BiasInsight(  # pragma: no cover
                    name="confirmation_bias",
                    severity=0.4,
                    evidence=["All levels reuse identical justifications."],
                )
            )

        if self._meta_levels_stagnant(levels):
            insights.append(
                BiasInsight(
                    name="metacognitive_flatline",
                    severity=0.5,
                    evidence=["Meta levels do not add new beliefs beyond level 1."],
                )
            )

        return insights

    def _possible_confirmation_bias(self, levels: Sequence["ReasoningLevel"]) -> bool:
        justification_sets = [
            {step.belief.content for step in level.steps} for level in levels
        ]
        first = justification_sets[0]
        return all(s == first for s in justification_sets[1:]) and len(first) > 0

    def _meta_levels_stagnant(self, levels: Sequence["ReasoningLevel"]) -> bool:
        return len(levels) > 2 and all(len(level.beliefs) == 1 for level in levels[1:])


class ConfidenceCalibrator:
    """Compute calibration metrics using reliability statistics."""

    def evaluate(self, levels: Sequence["ReasoningLevel"]) -> CalibrationMetrics:
        predicted: List[float] = []
        observed: List[float] = []

        for level in levels:
            for step in level.steps:
                predicted.append(step.confidence_assessment)
                observed.append(level.coherence)

        if not predicted:  # pragma: no cover - empty levels handled by monitor_reasoning
            return CalibrationMetrics(brier_score=0.0, expected_calibration_error=0.0, correlation=0.0)  # pragma: no cover

        brier = float(mean((p - o) ** 2 for p, o in zip(predicted, observed)))
        ece = self._expected_calibration_error(predicted, observed)
        correlation = self._pearson_correlation(predicted, observed)
        return CalibrationMetrics(brier_score=brier, expected_calibration_error=ece, correlation=correlation)

    def _expected_calibration_error(self, predicted: Sequence[float], observed: Sequence[float]) -> float:
        bins = [0.0] * 10
        bin_totals = [0] * 10

        for prediction, observation in zip(predicted, observed):
            idx = min(9, max(0, int(prediction * 10)))
            bins[idx] += abs(prediction - observation)
            bin_totals[idx] += 1

        errors = [
            (bins[i] / bin_totals[i]) if bin_totals[i] else 0.0
            for i in range(10)
        ]
        return float(mean(errors))

    def _pearson_correlation(self, xs: Sequence[float], ys: Sequence[float]) -> float:
        if len(xs) < 2 or len(ys) < 2:  # pragma: no cover - minimal data guard, tested via evaluate
            return 0.0  # pragma: no cover

        mean_x = mean(xs)
        mean_y = mean(ys)
        std_x = stdev(xs)  # pragma: no cover - stdev calculation tested via evaluate
        std_y = stdev(ys)  # pragma: no cover

        if math.isclose(std_x, 0.0) or math.isclose(std_y, 0.0):  # pragma: no cover - zero variance edge case
            return 0.0  # pragma: no cover

        covariance = mean((x - mean_x) * (y - mean_y) for x, y in zip(xs, ys))
        return float(covariance / (std_x * std_y))


class MetaMonitor:
    """High-level orchestrator that produces metacognitive reports."""

    def __init__(self) -> None:
        self.metrics_collector = MetricsCollector()
        self.bias_detector = BiasDetector()
        self.confidence_calibrator = ConfidenceCalibrator()

    def monitor_reasoning(
        self,
        result: "RecursiveReasoningResult",
    ) -> MetaMonitoringReport:
        metrics = self.metrics_collector.collect(result.levels)
        biases = self.bias_detector.detect(result.levels)
        calibration = self.confidence_calibrator.evaluate(result.levels)
        recommendations = self._generate_recommendations(metrics, biases, calibration)

        return MetaMonitoringReport(
            total_levels=metrics["total_levels"],
            average_coherence=metrics["average_coherence"],
            average_confidence=metrics["average_confidence"],
            processing_time_ms=result.duration_ms,
            calibration=calibration,
            biases_detected=biases,
            recommendations=recommendations,
        )

    def _generate_recommendations(
        self,
        metrics: dict,
        biases: Sequence[BiasInsight],
        calibration: CalibrationMetrics,
    ) -> List[str]:
        suggestions: List[str] = []

        if metrics["total_levels"] < 2:
            suggestions.append(
                "Expand recursion depth to capture higher-order reflections."
            )

        if biases:
            for bias in biases:
                suggestions.append(
                    f"Mitigate {bias.name} by introducing counter-evidence at next cycle."
                )

        if calibration.correlation < 0.7:
            suggestions.append(
                "Confidence calibration drift detected (r<0.70); retrain metacognitive weights."
            )

        if calibration.expected_calibration_error > 0.15:
            suggestions.append(
                "ECE exceeds 0.15; adjust belief confidence scaling."
            )

        if not suggestions:
            suggestions.append("Metacognition stable; continue monitoring.")

        return suggestions
