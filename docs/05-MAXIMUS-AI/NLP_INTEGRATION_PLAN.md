# üß† MAXIMUS NLP INTEGRATION - Plano Arquitetural

## üìç Status Atual

### Frontend: ‚úÖ 100% COMPLETO
- Interface chat estilo Claude implementada
- Mock API funcional com respostas inteligentes
- Persist√™ncia localStorage
- Zero warnings no console
- Pronto para integra√ß√£o backend

### Backend: üîÑ AGUARDANDO DEFINI√á√ÉO ARQUITETURAL

---

## üéØ Objetivo Central

**Primeira Intera√ß√£o Real com Maximus AI**

Criar um canal de comunica√ß√£o NLP que permita:
- Consultar status dos sistemas (Consciousness, Or√°culo, ADW, etc.)
- Solicitar an√°lises complexas
- Executar comandos via linguagem natural
- Receber insights do Maximus sobre seguran√ßa/opera√ß√µes

---

## üèóÔ∏è Op√ß√µes Arquiteturais

### **Op√ß√£o 1: NLP via LLM Externo (GPT/Claude/Gemini)**

**Arquitetura:**
```
User Input (Frontend)
    ‚Üì
NLP Processor (LLM API)
    ‚Üì
Intent Classifier
    ‚Üì
Maximus Internal APIs
    ‚Üì
Response Generator (LLM)
    ‚Üì
Frontend (formatted response)
```

**Pr√≥s:**
- ‚úÖ Compreens√£o de linguagem natural sofisticada
- ‚úÖ Gera√ß√£o de respostas contextuais
- ‚úÖ R√°pida implementa√ß√£o
- ‚úÖ Suporta multi-turn conversations

**Contras:**
- ‚ùå Depend√™ncia de servi√ßo externo
- ‚ùå Lat√™ncia de rede
- ‚ùå Custo por requisi√ß√£o
- ‚ùå Dados sens√≠veis passam por terceiros
- ‚ùå Precisa fine-tuning para comandos espec√≠ficos

**Stack Sugerido:**
```python
# backend/services/nlp_service/
‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îú‚îÄ‚îÄ gpt_processor.py      # OpenAI GPT-4
‚îÇ   ‚îú‚îÄ‚îÄ claude_processor.py   # Anthropic Claude
‚îÇ   ‚îî‚îÄ‚îÄ gemini_processor.py   # Google Gemini
‚îú‚îÄ‚îÄ intent_classifier.py
‚îú‚îÄ‚îÄ command_executor.py
‚îî‚îÄ‚îÄ response_formatter.py
```

---

### **Op√ß√£o 2: vcli-go NLP Engine (100% Local)**

**Arquitetura:**
```
User Input (Frontend)
    ‚Üì
vcli-go NLP Parser
    ‚Üì
Intent Extraction
    ‚Üì
Maximus Internal APIs
    ‚Üì
Template-based Response
    ‚Üì
Frontend
```

**Pr√≥s:**
- ‚úÖ 100% local (zero lat√™ncia externa)
- ‚úÖ Zero custo operacional
- ‚úÖ Controle total sobre l√≥gica
- ‚úÖ Dados sens√≠veis n√£o saem do sistema
- ‚úÖ vcli-go j√° implementado

**Contras:**
- ‚ùå Compreens√£o de linguagem limitada
- ‚ùå Respostas menos "naturais"
- ‚ùå Requer manuten√ß√£o de patterns/intents
- ‚ùå Dificuldade com ambiguidade

**Stack Atual (vcli-go):**
```go
// vcli-go/cmd/nlp/
‚îú‚îÄ‚îÄ parser.go          # NLP parsing
‚îú‚îÄ‚îÄ intent.go          # Intent detection
‚îú‚îÄ‚îÄ entities.go        # Entity extraction
‚îî‚îÄ‚îÄ response.go        # Response generation
```

---

### **Op√ß√£o 3: H√≠brida (vcli-go + LLM)**

**Arquitetura:**
```
User Input
    ‚Üì
vcli-go Intent Classifier (local, fast)
    ‚Üì
    ‚îú‚îÄ‚îÄ Simple Query ‚Üí vcli-go (local response)
    ‚îî‚îÄ‚îÄ Complex Query ‚Üí LLM API (enhanced response)
         ‚Üì
    Maximus Internal APIs
         ‚Üì
    Response (local or LLM-generated)
```

**Pr√≥s:**
- ‚úÖ Melhor custo-benef√≠cio
- ‚úÖ Queries simples 100% local
- ‚úÖ Queries complexas com LLM
- ‚úÖ Fallback inteligente
- ‚úÖ Privacidade para dados sens√≠veis

**Contras:**
- ‚ùå Complexidade de implementa√ß√£o
- ‚ùå Decis√£o de roteamento cr√≠tica
- ‚ùå Dois sistemas para manter

**Stack Proposto:**
```python
# backend/services/nlp_hybrid/
‚îú‚îÄ‚îÄ router.py                 # Decision: local vs LLM
‚îú‚îÄ‚îÄ vcli_adapter.py           # vcli-go integration
‚îú‚îÄ‚îÄ llm_adapter.py            # LLM integration
‚îî‚îÄ‚îÄ intent_complexity.py      # Query complexity analyzer
```

---

## üîç An√°lise de Use Cases

### Use Case 1: Status Query (Simples)
**Input:** "Qual o status do Consciousness Core?"

**vcli-go (local):**
```
Parse: intent=status, entity=consciousness_core
Query: GET /api/consciousness/status
Response: Template-based
Lat√™ncia: ~50ms
```

**LLM (external):**
```
Parse: GPT-4 interpreta contexto
Query: GET /api/consciousness/status
Response: Natural language gerada
Lat√™ncia: ~2000ms
```

**Recomenda√ß√£o:** ‚úÖ vcli-go suficiente

---

### Use Case 2: Complex Analysis (Complexa)
**Input:** "Analise os padr√µes de ataque das √∫ltimas 24h e sugira melhorias na postura defensiva"

**vcli-go (local):**
```
Parse: intent=analyze, timeframe=24h
Limita√ß√£o: N√£o consegue "sugerir melhorias" contextualmente
Response: Template gen√©rico
```

**LLM (external):**
```
Parse: Entende contexto + temporal + objetivo
Query: M√∫ltiplas APIs (ADW, logs, metrics)
Response: An√°lise narrativa + sugest√µes espec√≠ficas
Lat√™ncia: ~5000ms (aceit√°vel para an√°lise complexa)
```

**Recomenda√ß√£o:** ‚úÖ LLM necess√°rio

---

## üé® Proposta de Implementa√ß√£o

### **Fase 1: Funda√ß√£o (vcli-go)**

**Objetivo:** Canal b√°sico funcional 100% local

**Entregas:**
1. Endpoint `/api/nlp/chat` conectado ao vcli-go
2. Intents b√°sicos:
   - `status_query` - Status de m√≥dulos
   - `list_*` - Listar recursos
   - `get_metrics` - M√©tricas do sistema
3. Template-based responses
4. Logging de queries (para treinar LLM depois)

**Tempo estimado:** 1-2 dias

---

### **Fase 2: LLM Enhancement (H√≠brido)**

**Objetivo:** Adicionar intelig√™ncia para queries complexas

**Entregas:**
1. Router de complexidade
2. Integra√ß√£o LLM (Claude recomendado para seguran√ßa)
3. System prompts do Maximus
4. Context injection (dados do sistema)
5. Response streaming (SSE)

**Tempo estimado:** 3-4 dias

---

### **Fase 3: Autonomia (Maximus Self-Aware)**

**Objetivo:** Maximus gera insights proativos

**Entregas:**
1. Background analysis agent
2. Proactive notifications
3. Multi-turn planning
4. Decision explanation (Lei Zero/Lei I compliance)

**Tempo estimado:** 1 semana

---

## üîê Considera√ß√µes de Seguran√ßa

### Dados Sens√≠veis
```
‚ùå N√ÉO ENVIAR PARA LLM EXTERNO:
- Credenciais
- IPs internos completos
- Payloads de vulnerabilidades
- PII (Personally Identifiable Information)

‚úÖ PERMITIDO:
- Status de sistemas (healthy/degraded)
- M√©tricas agregadas
- Padr√µes de ataque (anonimizados)
- Logs sanitizados
```

### Sanitiza√ß√£o
```python
# backend/services/nlp_service/sanitizer.py

def sanitize_for_llm(data: dict) -> dict:
    """Remove sensitive data before sending to external LLM"""
    sanitized = data.copy()
    
    # Remove IPs
    sanitized = redact_ips(sanitized)
    
    # Aggregate metrics
    sanitized = aggregate_sensitive_metrics(sanitized)
    
    # Hash identifiers
    sanitized = hash_identifiers(sanitized)
    
    return sanitized
```

---

## üìä Matriz de Decis√£o

| Crit√©rio | vcli-go | LLM | H√≠brido |
|----------|---------|-----|---------|
| **Privacidade** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Lat√™ncia** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Compreens√£o NLP** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Custo** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Manuten√ß√£o** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Complexidade** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

---

## üí° Recomenda√ß√£o Final

### **Abordagem H√≠brida Incremental**

**Fase 1 (Imediata):** vcli-go puro
- Validar conceito
- Coletar dados de uso
- Identificar patterns complexos

**Fase 2 (2 semanas):** Adicionar LLM
- Implementar router
- Claude API (melhor custo/privacidade)
- Apenas para queries >threshold

**Fase 3 (1 m√™s):** Autonomia
- Maximus proativo
- Self-aware responses
- Constitutional compliance explanation

---

## üöÄ Pr√≥ximos Passos (Aguardando Co-Arquiteto)

### Decis√µes Necess√°rias

1. **LLM Provider?**
   - [ ] OpenAI GPT-4
   - [ ] Anthropic Claude
   - [ ] Google Gemini
   - [ ] Outro (especificar)

2. **Hosting?**
   - [ ] API keys (cloud)
   - [ ] Self-hosted (Ollama, LocalAI)
   - [ ] H√≠brido

3. **Threshold de Complexidade?**
   - [ ] Manual (lista de intents)
   - [ ] ML-based (similarity score)
   - [ ] Rule-based (keyword count)

4. **Data Privacy Policy?**
   - [ ] Definir lista de dados permitidos/proibidos
   - [ ] Implementar sanitizer
   - [ ] Audit logging

5. **Budget?**
   - [ ] Estimar custo mensal com LLM
   - [ ] Definir rate limits

---

## üìù Notas de Implementa√ß√£o

### Endpoint Proposto
```python
# backend/api/routes/nlp.py

@router.post("/api/nlp/chat")
async def chat(request: ChatRequest):
    """
    Main NLP endpoint for Maximus interaction
    
    Request:
        conversationId: str
        message: str
        context: dict (optional)
    
    Response:
        message: str
        sources: list[str]
        confidence: float
        processing_time: float
    """
    pass
```

### vcli-go Integration Point
```python
# backend/services/nlp_service/vcli_adapter.py

async def query_vcli(user_input: str) -> VCLIResponse:
    """
    Call vcli-go NLP parser
    
    Uses existing vcli-go implementation:
    - Intent detection
    - Entity extraction
    - Command routing
    """
    result = subprocess.run(
        ["./vcli-go", "nlp", "parse", user_input],
        capture_output=True
    )
    return parse_vcli_output(result.stdout)
```

---

**Criado em:** 2025-10-19  
**Status:** üîÑ AWAITING ARCHITECTURE DECISION  
**Frontend:** ‚úÖ READY  
**Backend:** ‚è≥ PENDING CO-ARCHITECT REVIEW
