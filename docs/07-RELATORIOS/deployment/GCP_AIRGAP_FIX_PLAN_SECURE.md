# ğŸ” PLANO DE CORREÃ‡ÃƒO AIR GAP - ARQUITETURA PRIVADA

**Data**: 2025-10-25 21:48 BRT  
**Objetivo**: Conectar Frontend (Cloud Run) â†’ Backend (GKE) via rede privada  
**PrincÃ­pio**: Zero exposiÃ§Ã£o pÃºblica atÃ© validaÃ§Ã£o completa  
**EstratÃ©gia**: VPC Connector + DNS Interno + Firewall Zero Trust

---

## ğŸ“‹ ARQUITETURA ALVO

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAMADA PÃšBLICA (Controlada)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cloud Run (Frontend)                                        â”‚
â”‚ â”œâ”€ HTTPS pÃºblico (ingress controlado por IAM)              â”‚
â”‚ â””â”€ VPC Egress via Connector (todo trÃ¡fego backend)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”‚ VPC Connector (10.8.0.0/28)
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAMADA PRIVADA (Zero acesso externo)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GKE Cluster (vertice-us-cluster)                            â”‚
â”‚ â”œâ”€ Services: ClusterIP (DNS interno)                        â”‚
â”‚ â”œâ”€ api-gateway.vertice.svc.cluster.local:8000              â”‚
â”‚ â”œâ”€ maximus-core-service.vertice.svc.cluster.local:8150     â”‚
â”‚ â”œâ”€ auth-service.vertice.svc.cluster.local:8006             â”‚
â”‚ â””â”€ service-registry.vertice.svc.cluster.local:8761         â”‚
â”‚                                                              â”‚
â”‚ Firewall Rules:                                             â”‚
â”‚ â”œâ”€ ALLOW: VPC Connector â†’ GKE Nodes                        â”‚
â”‚ â”œâ”€ DENY: * â†’ GKE (default)                                 â”‚
â”‚ â””â”€ ALLOW: GKE internal â†” GKE internal                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ FASE 1: PREPARAÃ‡ÃƒO E VALIDAÃ‡ÃƒO (15 min)

### 1.1 Validar Estado Atual GKE
**Objetivo**: Confirmar que backend estÃ¡ funcional internamente

```bash
# Verificar pods funcionais
kubectl get pods -n vertice -o wide

# Testar conectividade interna (de dentro do cluster)
kubectl run -n vertice test-curl --rm -it --image=curlimages/curl --restart=Never -- \
  curl -v http://api-gateway.vertice.svc.cluster.local:8000/health

# Validar DNS interno
kubectl run -n vertice test-dns --rm -it --image=busybox --restart=Never -- \
  nslookup api-gateway.vertice.svc.cluster.local
```

**CritÃ©rio de Sucesso**:
- âœ… Todos os pods `Running` (16/16)
- âœ… DNS interno resolve corretamente
- âœ… `/health` endpoint responde HTTP 200

**Bloqueador**: Se falhar, corrigir antes de prosseguir (rollback deploy se necessÃ¡rio)

---

### 1.2 Auditar ConfiguraÃ§Ã£o de Rede GKE
**Objetivo**: Confirmar que cluster estÃ¡ em VPC padrÃ£o e range de IPs

```bash
# Verificar VPC e subnet
gcloud container clusters describe vertice-us-cluster \
  --region=us-east1 \
  --format="value(network,subnetwork,clusterIpv4Cidr)"

# Verificar firewall rules existentes
gcloud compute firewall-rules list \
  --filter="network:default AND direction:INGRESS" \
  --format="table(name,sourceRanges,allowed)"

# Confirmar que GKE nÃ£o tem IP externo
kubectl get svc -n vertice -o wide | grep LoadBalancer
# Esperado: Nenhum resultado (ou EXTERNAL-IP = <none>)
```

**CritÃ©rio de Sucesso**:
- âœ… Network: `default` (ou VPC customizada identificada)
- âœ… Nenhum serviÃ§o com IP externo
- âœ… Firewall rules padrÃ£o GKE presentes

---

## ğŸ”§ FASE 2: CRIAÃ‡ÃƒO DO VPC CONNECTOR (20 min)

### 2.1 Criar Serverless VPC Access Connector
**Objetivo**: Ponte privada Cloud Run â†’ VPC

```bash
# Criar connector (range isolado, sem conflito com GKE)
gcloud compute networks vpc-access connectors create vertice-private-connector \
  --project=projeto-vertice \
  --region=us-east1 \
  --network=default \
  --range=10.8.0.0/28 \
  --min-instances=2 \
  --max-instances=10 \
  --machine-type=e2-micro

# Aguardar provisionamento (5-10 min)
gcloud compute networks vpc-access connectors describe vertice-private-connector \
  --region=us-east1 \
  --format="value(state)"
# Esperado: READY
```

**ValidaÃ§Ã£o**:
```bash
# Confirmar subnet criada
gcloud compute networks subnets list \
  --filter="name~vertice-private-connector" \
  --format="table(name,region,ipCidrRange)"
```

**CritÃ©rio de Sucesso**:
- âœ… Connector `state=READY`
- âœ… Subnet `10.8.0.0/28` criada
- âœ… InstÃ¢ncias min=2 rodando

**Custo**: ~$28/mÃªs (2 instÃ¢ncias e2-micro + dados)

---

### 2.2 Configurar Firewall Rules para Connector
**Objetivo**: Permitir APENAS trÃ¡fego Connector â†’ GKE

```bash
# Obter range de IPs dos nodes GKE
GKE_NODE_RANGE=$(gcloud container clusters describe vertice-us-cluster \
  --region=us-east1 \
  --format="value(clusterIpv4Cidr)")

# Regra 1: Permitir Connector â†’ GKE Nodes (porta 8000, 8006, 8150, 8761)
gcloud compute firewall-rules create allow-connector-to-gke \
  --network=default \
  --priority=1000 \
  --direction=INGRESS \
  --action=ALLOW \
  --source-ranges=10.8.0.0/28 \
  --target-tags=gke-vertice-us-cluster \
  --rules=tcp:8000,tcp:8006,tcp:8150,tcp:8761 \
  --description="Permite Cloud Run via VPC Connector acessar backend GKE"

# Regra 2: Negar qualquer outro acesso externo aos serviÃ§os backend
gcloud compute firewall-rules create deny-external-to-backend \
  --network=default \
  --priority=1100 \
  --direction=INGRESS \
  --action=DENY \
  --source-ranges=0.0.0.0/0 \
  --target-tags=gke-vertice-us-cluster \
  --rules=tcp:8000,tcp:8006,tcp:8150,tcp:8761 \
  --description="Bloqueia acesso externo direto aos serviÃ§os backend"
```

**ValidaÃ§Ã£o**:
```bash
# Listar regras criadas
gcloud compute firewall-rules list \
  --filter="name:(allow-connector-to-gke OR deny-external-to-backend)" \
  --format="table(name,direction,priority,sourceRanges,allowed,denied)"
```

**CritÃ©rio de Sucesso**:
- âœ… Regra ALLOW (priority 1000) para Connector
- âœ… Regra DENY (priority 1100) para resto do mundo
- âœ… Target tags correspondem aos nodes GKE

---

## ğŸš€ FASE 3: INTEGRAÃ‡ÃƒO CLOUD RUN â†’ VPC (15 min)

### 3.1 Configurar Cloud Run para usar VPC Connector
**Objetivo**: Rotear todo trÃ¡fego de egress via VPC privada

```bash
# Atualizar serviÃ§o Cloud Run
gcloud run services update vertice-frontend \
  --project=projeto-vertice \
  --region=us-east1 \
  --vpc-connector=vertice-private-connector \
  --vpc-egress=all-traffic \
  --no-allow-unauthenticated

# Confirmar configuraÃ§Ã£o
gcloud run services describe vertice-frontend \
  --region=us-east1 \
  --format="value(spec.template.spec.containers[0].env,spec.template.metadata.annotations)"
```

**Nota**: `--no-allow-unauthenticated` requer IAM token para acesso (adicionar depois se precisar pÃºblico)

**CritÃ©rio de Sucesso**:
- âœ… VPC connector configurado
- âœ… Egress = `all-traffic` (nÃ£o `private-ranges-only`)
- âœ… Deploy bem-sucedido sem erros

---

### 3.2 Testar Conectividade Cloud Run â†’ GKE (DiagnÃ³stico)
**Objetivo**: Validar que Cloud Run consegue resolver DNS interno GKE

```bash
# Deploy temporÃ¡rio de debug container no Cloud Run
cat > /tmp/debug-cloudrun.yaml <<EOF
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: vertice-debug
spec:
  template:
    metadata:
      annotations:
        run.googleapis.com/vpc-access-connector: vertice-private-connector
        run.googleapis.com/vpc-access-egress: all-traffic
    spec:
      containers:
      - image: curlimages/curl:latest
        command: ["/bin/sh"]
        args: ["-c", "while true; do sleep 3600; done"]
EOF

gcloud run services replace /tmp/debug-cloudrun.yaml \
  --region=us-east1 \
  --platform=managed

# Executar comando de teste
gcloud run services proxy vertice-debug --region=us-east1 &
PROXY_PID=$!
sleep 5

# Testar DNS e conectividade
curl -v "http://localhost:8080/exec?cmd=nslookup%20api-gateway.vertice.svc.cluster.local"
curl -v "http://localhost:8080/exec?cmd=curl%20-I%20http://api-gateway.vertice.svc.cluster.local:8000/health"

kill $PROXY_PID
```

**Alternativa (mais simples)**:
```bash
# Usar Cloud Shell para testar de dentro da VPC
gcloud compute instances create test-vpc-access \
  --zone=us-east1-b \
  --machine-type=e2-micro \
  --network=default \
  --subnet=default \
  --scopes=cloud-platform

# SSH e testar
gcloud compute ssh test-vpc-access --zone=us-east1-b --command="
  curl -v http://api-gateway.vertice.svc.cluster.local:8000/health
"

# Limpar
gcloud compute instances delete test-vpc-access --zone=us-east1-b --quiet
```

**CritÃ©rio de Sucesso**:
- âœ… DNS resolve `api-gateway.vertice.svc.cluster.local`
- âœ… HTTP 200 de `/health`
- âœ… LatÃªncia < 100ms

**Bloqueador**: Se falhar, verificar:
- Firewall rules aplicadas corretamente
- GKE nodes tÃªm network tag correto
- VPC Connector em `READY`

---

## ğŸ¨ FASE 4: REBUILD FRONTEND COM DNS INTERNO (30 min)

### 4.1 Atualizar ConfiguraÃ§Ã£o Frontend
**Objetivo**: Substituir `localhost` por DNS interno K8s

```bash
cd /home/juan/vertice-dev/frontend

# Criar arquivo .env.production
cat > .env.production <<EOF
# Backend URLs (DNS interno K8s)
VITE_API_GATEWAY_URL=http://api-gateway.vertice.svc.cluster.local:8000
VITE_AUTH_SERVICE_URL=http://auth-service.vertice.svc.cluster.local:8006
VITE_MAXIMUS_CORE_URL=http://maximus-core-service.vertice.svc.cluster.local:8150
VITE_SERVICE_REGISTRY_URL=http://service-registry.vertice.svc.cluster.local:8761

# Feature flags
VITE_ENABLE_DEBUG=false
VITE_ENVIRONMENT=production-private
EOF
```

**ValidaÃ§Ã£o**:
```bash
# Verificar que variÃ¡veis estÃ£o corretas
cat .env.production | grep -E "(API_GATEWAY|AUTH_SERVICE|MAXIMUS)"
```

---

### 4.2 Build e Deploy Frontend
**Objetivo**: Gerar bundle com URLs internas hardcoded

```bash
cd /home/juan/vertice-dev/frontend

# Build com env vars
npm run build

# Verificar que bundle NÃƒO tem localhost
grep -r "localhost" dist/ && echo "âŒ ERRO: localhost ainda presente!" || echo "âœ… OK"

# Verificar que bundle TEM DNS interno
grep -r "vertice.svc.cluster.local" dist/ && echo "âœ… DNS interno presente" || echo "âŒ ERRO"

# Build imagem Docker
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
docker build -t gcr.io/projeto-vertice/vertice-frontend:private-${TIMESTAMP} .

# Push para GCR
docker push gcr.io/projeto-vertice/vertice-frontend:private-${TIMESTAMP}

# Deploy no Cloud Run
gcloud run deploy vertice-frontend \
  --project=projeto-vertice \
  --region=us-east1 \
  --image=gcr.io/projeto-vertice/vertice-frontend:private-${TIMESTAMP} \
  --vpc-connector=vertice-private-connector \
  --vpc-egress=all-traffic \
  --set-env-vars="NODE_ENV=production" \
  --no-allow-unauthenticated \
  --min-instances=1 \
  --max-instances=10 \
  --memory=512Mi \
  --cpu=1 \
  --timeout=60s
```

**CritÃ©rio de Sucesso**:
- âœ… Build sem erros
- âœ… Zero ocorrÃªncias de `localhost` em `dist/`
- âœ… Deploy Cloud Run HTTP 200

---

## âœ… FASE 5: VALIDAÃ‡ÃƒO END-TO-END (20 min)

### 5.1 Teste de Conectividade Completo
**Objetivo**: Confirmar fluxo Frontend â†’ Backend funcional

```bash
# Obter URL do Cloud Run
FRONTEND_URL=$(gcloud run services describe vertice-frontend \
  --region=us-east1 \
  --format="value(status.url)")

# Gerar token de autenticaÃ§Ã£o (necessÃ¡rio para Cloud Run privado)
TOKEN=$(gcloud auth print-identity-token)

# Teste 1: Frontend carrega
curl -H "Authorization: Bearer $TOKEN" "$FRONTEND_URL" -I

# Teste 2: Frontend consegue chamar backend (via browser DevTools ou proxy)
# Executar no Cloud Shell (mesmo contexto VPC)
gcloud run services proxy vertice-frontend --region=us-east1 &
PROXY_PID=$!
sleep 5

# Simular request do frontend
curl -H "Authorization: Bearer $TOKEN" \
  "http://localhost:8080" \
  -H "X-Test-Backend: true" \
  -v

kill $PROXY_PID
```

---

### 5.2 Verificar Logs de Conectividade
**Objetivo**: Confirmar que requests chegam no backend GKE

```bash
# Logs Cloud Run (frontend)
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=vertice-frontend" \
  --limit=50 \
  --format=json | jq '.[] | select(.jsonPayload.message | contains("api-gateway"))'

# Logs GKE (backend - api-gateway)
kubectl logs -n vertice -l app=api-gateway --tail=100 | grep -E "(GET|POST|PUT|DELETE)"

# Verificar se hÃ¡ requests vindos do range do VPC Connector (10.8.0.0/28)
kubectl logs -n vertice -l app=api-gateway --tail=100 | grep "10.8.0"
```

**CritÃ©rio de Sucesso**:
- âœ… Logs frontend mostram requests para `*.svc.cluster.local`
- âœ… Logs backend mostram requests com source IP `10.8.0.*`
- âœ… Zero erros de DNS resolution
- âœ… Zero timeout errors

---

### 5.3 Teste de SeguranÃ§a (Validar Isolamento)
**Objetivo**: Confirmar que backend NÃƒO Ã© acessÃ­vel externamente

```bash
# Tentar acessar backend diretamente (deve falhar)
# Obter IP de um node GKE
GKE_NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="ExternalIP")].address}')

# Teste 1: Tentar porta 8000 (deve ser bloqueada)
curl -v --connect-timeout 5 http://${GKE_NODE_IP}:8000/health
# Esperado: Connection refused ou timeout

# Teste 2: Confirmar que nÃ£o hÃ¡ serviÃ§os LoadBalancer
kubectl get svc -n vertice -o json | jq '.items[] | select(.spec.type=="LoadBalancer")'
# Esperado: Vazio

# Teste 3: Confirmar que nÃ£o hÃ¡ Ingress pÃºblico
kubectl get ingress -A
# Esperado: No resources found
```

**CritÃ©rio de Sucesso**:
- âœ… Nenhuma porta backend acessÃ­vel externamente
- âœ… Firewall rules negam acesso externo
- âœ… Zero serviÃ§os LoadBalancer
- âœ… Zero Ingress configurados

---

## ğŸ“Š FASE 6: MONITORAMENTO E OBSERVABILIDADE (15 min)

### 6.1 Configurar Alertas de Conectividade
**Objetivo**: Detectar falhas na ponte VPC

```bash
# Alerta 1: VPC Connector saturado (>80% utilizaÃ§Ã£o)
gcloud alpha monitoring policies create \
  --notification-channels=<CHANNEL_ID> \
  --display-name="VPC Connector Alta UtilizaÃ§Ã£o" \
  --condition-display-name="Connector >80% capacity" \
  --condition-threshold-value=0.8 \
  --condition-threshold-duration=300s \
  --condition-filter='resource.type="vpc_access_connector" AND metric.type="vpcaccess.googleapis.com/connector/connections"'

# Alerta 2: Cloud Run â†’ Backend latÃªncia alta (>500ms)
gcloud alpha monitoring policies create \
  --notification-channels=<CHANNEL_ID> \
  --display-name="LatÃªncia Frontendâ†’Backend Alta" \
  --condition-display-name="Latency >500ms" \
  --condition-threshold-value=500 \
  --condition-threshold-duration=60s \
  --condition-filter='resource.type="cloud_run_revision" AND metric.type="run.googleapis.com/request_latencies"'
```

---

### 6.2 Dashboard de Conectividade Privada
**Objetivo**: Visibilidade centralizada do fluxo privado

```bash
# Criar dashboard customizado
cat > /tmp/vpc-connectivity-dashboard.json <<EOF
{
  "displayName": "VÃ©rtice - Conectividade Privada",
  "gridLayout": {
    "widgets": [
      {
        "title": "VPC Connector - ConexÃµes Ativas",
        "xyChart": {
          "dataSets": [{
            "timeSeriesQuery": {
              "timeSeriesFilter": {
                "filter": "resource.type=\"vpc_access_connector\" AND metric.type=\"vpcaccess.googleapis.com/connector/connections\""
              }
            }
          }]
        }
      },
      {
        "title": "Cloud Run - Requests",
        "xyChart": {
          "dataSets": [{
            "timeSeriesQuery": {
              "timeSeriesFilter": {
                "filter": "resource.type=\"cloud_run_revision\" AND metric.type=\"run.googleapis.com/request_count\""
              }
            }
          }]
        }
      },
      {
        "title": "GKE Backend - HTTP 5xx",
        "xyChart": {
          "dataSets": [{
            "timeSeriesQuery": {
              "timeSeriesFilter": {
                "filter": "resource.type=\"k8s_pod\" AND metric.type=\"kubernetes.io/container/http_5xx_count\""
              }
            }
          }]
        }
      }
    ]
  }
}
EOF

gcloud monitoring dashboards create --config-from-file=/tmp/vpc-connectivity-dashboard.json
```

---

## ğŸ” FASE 7: HARDENING DE SEGURANÃ‡A (10 min)

### 7.1 Configurar IAM Restritivo
**Objetivo**: Acesso ao frontend apenas para usuÃ¡rios autorizados

```bash
# Remover acesso pÃºblico (jÃ¡ feito com --no-allow-unauthenticated)
gcloud run services remove-iam-policy-binding vertice-frontend \
  --region=us-east1 \
  --member="allUsers" \
  --role="roles/run.invoker" \
  2>/dev/null || true

# Adicionar usuÃ¡rios/SAs especÃ­ficos
gcloud run services add-iam-policy-binding vertice-frontend \
  --region=us-east1 \
  --member="user:admin@vertice.com" \
  --role="roles/run.invoker"

# Para acesso via gcloud auth (desenvolvimento)
gcloud run services add-iam-policy-binding vertice-frontend \
  --region=us-east1 \
  --member="serviceAccount:$(gcloud config get-value account)" \
  --role="roles/run.invoker"
```

---

### 7.2 Audit Logging
**Objetivo**: Registrar todos os acessos ao sistema

```bash
# Habilitar audit logs para Cloud Run
cat > /tmp/audit-config.yaml <<EOF
auditConfigs:
- service: run.googleapis.com
  auditLogConfigs:
  - logType: ADMIN_READ
  - logType: DATA_READ
  - logType: DATA_WRITE
EOF

gcloud projects set-iam-policy projeto-vertice /tmp/audit-config.yaml
```

---

## ğŸ“ CHECKLIST FINAL

### PrÃ©-Deploy
- [ ] GKE pods 16/16 Running
- [ ] DNS interno K8s funcional
- [ ] Nenhum serviÃ§o GKE exposto publicamente

### Deploy VPC Connector
- [ ] VPC Connector criado (state=READY)
- [ ] Firewall rule ALLOW (Connector â†’ GKE)
- [ ] Firewall rule DENY (Internet â†’ GKE)

### Deploy Cloud Run
- [ ] Frontend buildado com DNS interno
- [ ] Zero ocorrÃªncias de `localhost` no bundle
- [ ] VPC Connector configurado
- [ ] IAM policy restritivo aplicado

### ValidaÃ§Ã£o
- [ ] Cloud Run consegue resolver DNS K8s
- [ ] Requests chegam no backend GKE
- [ ] Logs mostram source IP 10.8.0.*
- [ ] Backend NÃƒO acessÃ­vel externamente

### Monitoramento
- [ ] Dashboard de conectividade criado
- [ ] Alertas de latÃªncia configurados
- [ ] Audit logs habilitados

---

## ğŸš¨ ROLLBACK PLAN

Se algo falhar, executar:

```bash
# Reverter Cloud Run para imagem anterior
gcloud run services update vertice-frontend \
  --region=us-east1 \
  --image=gcr.io/projeto-vertice/vertice-frontend:PREVIOUS_TAG

# Remover VPC Connector (se necessÃ¡rio)
gcloud compute networks vpc-access connectors delete vertice-private-connector \
  --region=us-east1 \
  --quiet

# Remover firewall rules
gcloud compute firewall-rules delete allow-connector-to-gke --quiet
gcloud compute firewall-rules delete deny-external-to-backend --quiet
```

---

## ğŸ“Š MÃ‰TRICAS DE SUCESSO

- âœ… **Conectividade**: Frontend â†’ Backend latÃªncia < 100ms
- âœ… **SeguranÃ§a**: Zero portas backend expostas publicamente
- âœ… **Disponibilidade**: 99.9% uptime (validar em 24h)
- âœ… **Performance**: VPC Connector utilizaÃ§Ã£o < 50%
- âœ… **Custo**: ~$30/mÃªs (VPC Connector + mÃ­nimas instÃ¢ncias Cloud Run)

---

## ğŸ“ ARTIFACTS GERADOS

- `/tmp/debug-cloudrun.yaml` - Debug container
- `/tmp/vpc-connectivity-dashboard.json` - Dashboard
- `/tmp/audit-config.yaml` - Audit logging
- `frontend/.env.production` - ConfiguraÃ§Ã£o DNS interno
- `gcr.io/projeto-vertice/vertice-frontend:private-YYYYMMDD-HHMMSS` - Imagem

---

**PRONTO PARA EXECUÃ‡ÃƒO**: Aguardando aprovaÃ§Ã£o do Arquiteto-Chefe.

**TEMPO ESTIMADO TOTAL**: 2h 5min

**JANELA DE RISCO**: Zero (nenhuma alteraÃ§Ã£o afeta infraestrutura existente atÃ© deploy final)
