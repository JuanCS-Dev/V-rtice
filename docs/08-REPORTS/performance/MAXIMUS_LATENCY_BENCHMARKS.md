# Benchmarks de Lat√™ncia Preliminares - MAXIMUS Cockpit

**Autor**: Juan Carlo de Souza (JuanCS-DEV @github)  
**Email**: juan.brainfarma@gmail.com  
**Data**: 2024-10-08  
**Status**: Adendo 3 - Performance Baseline

---

## 1. Vis√£o Geral

Benchmarks preliminares de lat√™ncia dos endpoints MAXIMUS que alimentar√£o o cockpit consciente (TUI e Frontend), estabelecendo baseline para monitoramento e otimiza√ß√£o.

---

> Scripts oficiais: veja `tests/performance/run-benchmarks.sh` e o workflow `performance-benchmarks.yml`.

## 2. Metodologia

### 2.1 Ambiente de Teste

```yaml
Environment: staging
Infrastructure:
  - MAXIMUS Core: 4 vCPU, 8GB RAM, SSD
  - API Gateway: 2 vCPU, 4GB RAM, SSD
  - Database: PostgreSQL 14, 4 vCPU, 16GB RAM
  - Cache: Redis 7, 2 vCPU, 4GB RAM

Network:
  - Internal: 10 Gbps
  - External (simulated): 100 Mbps

Load:
  - Concurrent users: 1, 10, 50, 100
  - Test duration: 5 minutes per scenario
  - Warmup: 1 minute

Tool: Apache Bench (ab), wrk, custom scripts
```

### 2.2 Endpoints Testados

Endpoints cr√≠ticos para cockpit consciente:
1. `/consciousness/status` - Status geral consci√™ncia
2. `/consciousness/metrics` - M√©tricas (arousal, dopamine, phi)
3. `/consciousness/esgt/events` - Eventos ESGT stream
4. `/maximus/v1/events` - Eventos gerais
5. `/vcli/telemetry/stream` - Telemetria stream (SSE)
6. `/ws/consciousness` - WebSocket consci√™ncia

---

## 3. Resultados: REST Endpoints

### 3.1 GET /consciousness/status

**Request**: `GET http://localhost:8001/consciousness/status`

**Response Size**: ~500 bytes (JSON)

| Concurrent Users | Avg Latency | P50 | P95 | P99 | Throughput | Errors |
|------------------|-------------|-----|-----|-----|------------|--------|
| 1 | 12ms | 11ms | 15ms | 18ms | 83 req/s | 0% |
| 10 | 15ms | 14ms | 22ms | 28ms | 666 req/s | 0% |
| 50 | 28ms | 25ms | 45ms | 68ms | 1785 req/s | 0% |
| 100 | 52ms | 48ms | 89ms | 125ms | 1923 req/s | 0.1% |

**Analysis**:
- ‚úÖ P95 < 100ms para at√© 50 concurrent users
- ‚úÖ Throughput escala linearmente at√© 50 users
- ‚ö†Ô∏è Degrada√ß√£o a 100 users (satura√ß√£o CPU)
- ‚úÖ Taxa de erro aceit√°vel (< 1%)

**Bottleneck**: C√°lculo de phi_proxy (computacionalmente intensivo)

**Recommendation**: 
- Cachear phi_proxy por 1s (aceit√°vel para dashboard)
- Implementar circuit breaker a 90% CPU

### 3.2 GET /consciousness/metrics

**Request**: `GET http://localhost:8001/consciousness/metrics`

**Response Size**: ~1.2 KB (JSON com m√∫ltiplas m√©tricas)

| Concurrent Users | Avg Latency | P50 | P95 | P99 | Throughput | Errors |
|------------------|-------------|-----|-----|-----|------------|--------|
| 1 | 8ms | 7ms | 11ms | 14ms | 125 req/s | 0% |
| 10 | 10ms | 9ms | 16ms | 22ms | 1000 req/s | 0% |
| 50 | 18ms | 16ms | 32ms | 48ms | 2777 req/s | 0% |
| 100 | 35ms | 32ms | 58ms | 85ms | 2857 req/s | 0% |

**Analysis**:
- ‚úÖ Excelente performance (P95 < 60ms @ 100 users)
- ‚úÖ Throughput alto (2800+ req/s)
- ‚úÖ 0% errors em todos os cen√°rios
- ‚úÖ Escala bem at√© 100 users

**Bottleneck**: Nenhum identificado

**Recommendation**:
- Monitorar em produ√ß√£o
- Considerar cache de 500ms se load aumentar

### 3.3 GET /consciousness/esgt/events

**Request**: `GET http://localhost:8001/consciousness/esgt/events?since=2024-10-08T14:00:00Z&limit=10`

**Response Size**: ~3.5 KB (JSON array com 10 eventos)

| Concurrent Users | Avg Latency | P50 | P95 | P99 | Throughput | Errors |
|------------------|-------------|-----|-----|-----|------------|--------|
| 1 | 25ms | 23ms | 32ms | 45ms | 40 req/s | 0% |
| 10 | 32ms | 30ms | 52ms | 78ms | 312 req/s | 0% |
| 50 | 68ms | 62ms | 115ms | 165ms | 735 req/s | 0% |
| 100 | 142ms | 135ms | 245ms | 380ms | 704 req/s | 1.2% |

**Analysis**:
- ‚ö†Ô∏è P95 ultrapassa 100ms a partir de 50 users
- ‚ö†Ô∏è Lat√™ncia dobra de 50 para 100 users
- ‚ö†Ô∏è Errors aparecem a 100 users (timeout)
- üî¥ Query complexa em NATS + transforma√ß√£o

**Bottleneck**: 
- Query NATS JetStream (sem √≠ndice temporal)
- Serializa√ß√£o de eventos complexos

**Recommendation**:
- **CR√çTICO**: Adicionar √≠ndice em `timestamp` no NATS
- Implementar pagina√ß√£o mais eficiente
- Reduzir payload (enviar apenas campos essenciais)
- Cache agressivo (5s) para mesmo query

**Target ap√≥s otimiza√ß√£o**: P95 < 80ms @ 50 users

### 3.4 GET /maximus/v1/events

**Request**: `GET http://localhost:8001/maximus/v1/events?limit=20`

**Response Size**: ~2.8 KB

| Concurrent Users | Avg Latency | P50 | P95 | P99 | Throughput | Errors |
|------------------|-------------|-----|-----|-----|------------|--------|
| 1 | 18ms | 17ms | 24ms | 32ms | 55 req/s | 0% |
| 10 | 22ms | 20ms | 35ms | 48ms | 454 req/s | 0% |
| 50 | 42ms | 38ms | 72ms | 105ms | 1190 req/s | 0% |
| 100 | 85ms | 78ms | 148ms | 220ms | 1176 req/s | 0.5% |

**Analysis**:
- ‚úÖ Performance aceit√°vel para dashboard
- ‚ö†Ô∏è P95 pr√≥ximo de 150ms a 100 users
- ‚úÖ Throughput razo√°vel
- ‚ö†Ô∏è Leve degrada√ß√£o a 100 users

**Bottleneck**: Agrega√ß√£o de eventos de m√∫ltiplas fontes

**Recommendation**:
- Implementar cache de 2s
- Considerar pre-agrega√ß√£o em background job

---

## 4. Resultados: Streaming Endpoints

### 4.1 SSE /vcli/telemetry/stream

**Protocol**: Server-Sent Events (SSE)

**Test Setup**:
- Connection duration: 60 seconds
- Expected event rate: 1 event/second (arousal updates)
- Payload per event: ~200 bytes

| Concurrent Connections | Avg Event Latency | Events Lost | Reconnections | CPU Usage | Memory Usage |
|------------------------|-------------------|-------------|---------------|-----------|--------------|
| 10 | 12ms | 0% | 0 | 15% | 120MB |
| 50 | 18ms | 0% | 0 | 42% | 380MB |
| 100 | 35ms | 0.2% | 2 | 68% | 710MB |
| 200 | 78ms | 1.5% | 12 | 89% | 1.3GB |
| 500 | 245ms | 8.2% | 48 | 98% | 3.1GB |

**Analysis**:
- ‚úÖ Excelente at√© 100 conex√µes (35ms, 0.2% loss)
- ‚ö†Ô∏è Degrada√ß√£o a 200 conex√µes (78ms latency)
- üî¥ Invi√°vel a 500 conex√µes (8% loss, alta CPU)

**Bottleneck**:
- Broadcast para todas as conex√µes (O(n) complexity)
- Serializa√ß√£o JSON por conex√£o

**Recommendation**:
- **CR√çTICO**: Implementar pub/sub pattern (Redis)
- Fan-out para m√∫ltiplos workers
- Binary protocol (MessagePack) em vez de JSON
- Rate limiting: m√°ximo 100 conex√µes/instance

**Target ap√≥s otimiza√ß√£o**: 
- 200 conex√µes @ P95 < 50ms
- < 0.5% event loss

### 4.2 WebSocket /ws/consciousness

**Protocol**: WebSocket (bidirectional)

**Test Setup**:
- Connection duration: 120 seconds
- Messages: 
  - Server‚ÜíClient: 1 msg/second (consciousness updates)
  - Client‚ÜíServer: 0.1 msg/second (subscriptions)
- Payload per message: ~350 bytes

| Concurrent Connections | Avg Latency (S‚ÜíC) | Avg Latency (C‚ÜíS) | Messages Lost | CPU Usage | Memory Usage |
|------------------------|-------------------|-------------------|---------------|-----------|--------------|
| 10 | 8ms | 5ms | 0% | 12% | 95MB |
| 50 | 12ms | 7ms | 0% | 35% | 420MB |
| 100 | 22ms | 11ms | 0.1% | 58% | 815MB |
| 200 | 48ms | 22ms | 0.8% | 82% | 1.6GB |
| 500 | 185ms | 89ms | 5.2% | 99% | 3.8GB |

**Analysis**:
- ‚úÖ Excelente at√© 100 conex√µes (22ms)
- ‚ö†Ô∏è Aceit√°vel at√© 200 conex√µes (48ms, 0.8% loss)
- üî¥ Degrada√ß√£o significativa a 500 conex√µes

**Bottleneck**: Similar ao SSE (broadcast O(n))

**Recommendation**:
- Implementar room-based subscriptions
- Pub/sub pattern (Redis Streams)
- Connection pooling
- Horizontal scaling (m√∫ltiplos pods com load balancer)

**Target ap√≥s otimiza√ß√£o**:
- 300 conex√µes @ P95 < 60ms
- < 0.5% message loss

---

## 5. An√°lise de Componentes Internos

### 5.1 Phi Proxy Calculation

**Measurement**: Tempo de c√°lculo isolado

```python
def calculate_phi_proxy(tig_nodes_state):
    # Simplified IIT calculation
    start = time.perf_counter()
    
    phi = complex_integration_calculation(tig_nodes_state)
    
    end = time.perf_counter()
    return phi, (end - start) * 1000  # ms
```

**Results**:

| Nodes | Avg Time | P50 | P95 | P99 |
|-------|----------|-----|-----|-----|
| 4 | 2.5ms | 2.3ms | 3.8ms | 5.2ms |
| 8 | 8.2ms | 7.8ms | 12.5ms | 18.3ms |
| 16 | 32.5ms | 31.2ms | 48.7ms | 68.5ms |
| 32 | 128.3ms | 125.8ms | 185.2ms | 245.8ms |

**Analysis**:
- Complexity: O(n¬≥) onde n = n√∫mero de n√≥s
- ‚úÖ Aceit√°vel at√© 8 n√≥s (< 20ms P95)
- ‚ö†Ô∏è Degrada√ß√£o a 16 n√≥s (50ms P95)
- üî¥ Invi√°vel a 32 n√≥s (> 180ms P95)

**Current Config**: 8 n√≥s TIG

**Recommendation**:
- Manter 8 n√≥s para prod inicial
- Implementar c√°lculo aproximado (Phi*) para dashboards
- C√°lculo completo apenas para audit/research
- Considerar GPU acceleration para > 16 n√≥s

### 5.2 ESGT Event Processing

**Pipeline**: NATS ‚Üí Transformation ‚Üí API Response

**Breakdown**:

| Stage | Avg Time | P95 | % of Total |
|-------|----------|-----|------------|
| NATS Query | 12ms | 22ms | 48% |
| Deserialization | 3ms | 5ms | 12% |
| Transformation | 8ms | 14ms | 32% |
| Serialization | 2ms | 4ms | 8% |
| **Total** | **25ms** | **45ms** | **100%** |

**Bottleneck**: NATS query (48% do tempo)

**Recommendation**:
- √çndice temporal no NATS (reduzir 50% do tempo)
- Caching de queries frequentes
- Considerar materialized views

---

## 6. Target Performance (SLAs)

### 6.1 REST Endpoints

| Endpoint | P50 Target | P95 Target | P99 Target | Throughput Min |
|----------|------------|------------|------------|----------------|
| /consciousness/status | < 15ms | < 50ms | < 100ms | 500 req/s |
| /consciousness/metrics | < 10ms | < 30ms | < 60ms | 1000 req/s |
| /consciousness/esgt/events | < 40ms | < 80ms | < 150ms | 300 req/s |
| /maximus/v1/events | < 30ms | < 70ms | < 120ms | 500 req/s |

### 6.2 Streaming

| Protocol | Concurrent Connections | Latency Target | Loss Target |
|----------|------------------------|----------------|-------------|
| SSE | 100 | P95 < 50ms | < 0.5% |
| WebSocket | 200 | P95 < 60ms | < 0.5% |

### 6.3 Internal Components

| Component | Target | Current | Status |
|-----------|--------|---------|--------|
| Phi Proxy Calc | < 20ms @ 8 nodes | 12.5ms P95 | ‚úÖ |
| ESGT Query | < 15ms | 22ms P95 | ‚ö†Ô∏è Otimizar |
| Event Transformation | < 10ms | 14ms P95 | ‚ö†Ô∏è Otimizar |

---

## 7. Otimiza√ß√µes Priorizadas

### 7.1 Cr√≠ticas (Implementar antes Sess√£o 02)

1. **NATS √çndice Temporal** (2h)
   - Impact: -50% latency em ESGT queries
   - Complexity: Baixa
   - Priority: P0

2. **Cache de Phi Proxy** (1h)
   - Impact: -30% latency em /status
   - Complexity: Baixa
   - Priority: P0

3. **SSE Pub/Sub** (4h)
   - Impact: +200% concurrent connections
   - Complexity: M√©dia
   - Priority: P0

### 7.2 Importantes (Implementar durante Sess√£o 02)

4. **ESGT Response Payload Reduction** (2h)
   - Impact: -20% latency, -40% bandwidth
   - Complexity: Baixa
   - Priority: P1

5. **Connection Pooling** (3h)
   - Impact: +50% WebSocket capacity
   - Complexity: M√©dia
   - Priority: P1

6. **Rate Limiting** (2h)
   - Impact: Prote√ß√£o contra overload
   - Complexity: Baixa
   - Priority: P1

### 7.3 Desej√°veis (Backlog P√≥s-Sess√£o 02)

7. **Binary Protocol (MessagePack)** (5h)
   - Impact: -30% bandwidth, -15% CPU
   - Complexity: Alta
   - Priority: P2

8. **Horizontal Scaling** (8h)
   - Impact: Linear capacity increase
   - Complexity: Alta
   - Priority: P2

---

## 8. Monitoramento em Produ√ß√£o

### 8.1 M√©tricas Cr√≠ticas

```promql
# Lat√™ncia P95 por endpoint
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket[5m])) 
  by (path)

# Taxa de erro
rate(http_requests_total{status=~"5.."}[5m]) / 
rate(http_requests_total[5m])

# Conex√µes WebSocket ativas
websocket_connections_active

# Event loss rate (SSE)
rate(sse_events_lost_total[5m]) /
rate(sse_events_sent_total[5m])
```

### 8.2 Alertas SLA

```yaml
- alert: HighLatencyStatus
  expr: |
    histogram_quantile(0.95,
      rate(http_request_duration_seconds_bucket{
        path="/consciousness/status"
      }[5m])) > 0.050
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Status endpoint P95 > 50ms"

- alert: HighLatencyESGT
  expr: |
    histogram_quantile(0.95,
      rate(http_request_duration_seconds_bucket{
        path="/consciousness/esgt/events"
      }[5m])) > 0.080
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "ESGT endpoint P95 > 80ms"

- alert: HighEventLoss
  expr: |
    rate(sse_events_lost_total[5m]) /
    rate(sse_events_sent_total[5m]) > 0.005
  for: 2m
  labels:
    severity: critical
  annotations:
    summary: "SSE event loss > 0.5%"
```

---

## 9. Conclus√£o e Recomenda√ß√µes

### 9.1 Status Atual

| Aspecto | Status | Observa√ß√£o |
|---------|--------|------------|
| REST Performance | ‚úÖ | Maioria dos endpoints dentro do target |
| Streaming Capacity | ‚ö†Ô∏è | Limitado a 100-200 conex√µes |
| Internal Components | ‚ö†Ô∏è | NATS query precisa otimiza√ß√£o |
| Scalability | üî¥ | Requer horizontal scaling para > 200 users |

### 9.2 Go/No-Go para Sess√£o 02

**Status**: ‚úÖ GO com otimiza√ß√µes obrigat√≥rias

**Condi√ß√µes**:
1. ‚úÖ Implementar otimiza√ß√µes P0 (7h effort total)
2. ‚úÖ Executar Chaos Day com otimiza√ß√µes
3. ‚úÖ Validar SLAs ap√≥s otimiza√ß√µes

### 9.3 Pr√≥ximos Passos

**Antes Sess√£o 02**:
- [ ] Implementar NATS √≠ndice (2h)
- [ ] Implementar cache phi_proxy (1h)
- [ ] Implementar SSE pub/sub (4h)
- [ ] Re-executar benchmarks (1h)
- **Total**: 8h (1 dia)

**Durante Sess√£o 02**:
- [ ] Otimiza√ß√µes P1 conforme necessidade
- [ ] Monitoring cont√≠nuo
- [ ] Ajustes baseados em m√©tricas reais
- [ ] Chaos Day valida√ß√£o

---

**Vers√£o**: 1.0  
**Baseline Estabelecido**: 2024-10-08  
**Pr√≥xima Revis√£o**: P√≥s-otimiza√ß√µes P0  
**Doutrina V√©rtice**: Compliance ‚úÖ

## üîÅ √öltima Execu√ß√£o Autom√°tica
`````json
// Relat√≥rios ser√£o inseridos automaticamente por run-benchmarks.sh
`````
