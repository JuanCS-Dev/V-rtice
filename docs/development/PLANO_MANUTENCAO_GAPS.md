# üîß PLANO DE MANUTEN√á√ÉO - ELIMINA√á√ÉO DE GAPS

**Data**: 2025-10-26  
**Objetivo**: Eliminar gaps detectados SEM adicionar features  
**Restri√ß√µes**: Zero custos adicionais, zero quebra de c√≥digo existente  
**Escopo**: APENAS manuten√ß√£o corretiva

---

## üìã GAPS IDENTIFICADOS

### ‚úÖ GAPS J√Å RESOLVIDOS (n√£o tocar)
- Frontend ‚Üí Backend integration (100% funcional)
- API Gateway routing (operacional)
- LoadBalancer √∫nico (arquitetura correta)

### ‚ùå GAPS CR√çTICOS ATIVOS

| ID | Gap | Impacto | Causa Raiz |
|----|-----|---------|------------|
| **GAP-1** | hitl-patch-service CrashLoop | HITL Dashboard DOWN | PostgreSQL connection failure |
| **GAP-2** | command-bus-service CrashLoop | Command routing degradado | A investigar |
| **GAP-3** | agent-communication CrashLoop | Inter-agent sync degradado | A investigar |
| **GAP-4** | 11 pods em CrashLoop | Funcionalidades parciais | Variadas |

---

## üéØ METODOLOGIA DE FIX

### Princ√≠pios
1. **Diagn√≥stico antes de a√ß√£o**: Logs ‚Üí Causa-raiz ‚Üí Fix cir√∫rgico
2. **Um gap por vez**: Fix ‚Üí Valida√ß√£o ‚Üí Pr√≥ximo
3. **Zero impacto**: N√£o mexer em c√≥digo funcional
4. **Rollback r√°pido**: Cada fix deve ser revers√≠vel

### Ordem de Execu√ß√£o
1. GAP-1 (CR√çTICO) - HITL service
2. GAP-2 (ALTO) - Command Bus
3. GAP-3 (ALTO) - Agent Communication
4. GAP-4 (M√âDIO) - Demais pods (priorizar por impacto)

---

## üìù PLANO DE EXECU√á√ÉO DETALHADO

---

## GAP-1: hitl-patch-service (CR√çTICO)

### Diagn√≥stico
```bash
# 1. Verificar logs do pod
kubectl logs -n vertice -l app=hitl-patch-service --tail=100

# 2. Verificar secret PostgreSQL
kubectl get secret -n vertice vertice-core-secrets -o jsonpath='{.data.POSTGRES_PASSWORD}' | base64 -d

# 3. Testar conex√£o PostgreSQL direta
kubectl exec -n vertice postgres-0 -- psql -U vertice -d vertice -c "SELECT 1"

# 4. Verificar env vars do deployment
kubectl get deployment -n vertice hitl-patch-service -o yaml | grep -A 20 "env:"
```

### Poss√≠veis Causas
1. Secret `POSTGRES_PASSWORD` incorreto/corrompido
2. PostgreSQL host/port incorreto (DNS resolution)
3. Database `vertice` n√£o existe
4. User `vertice` sem permiss√µes

### Fix Candidato 1: Validar Secret
```bash
# Se password incorreto:
kubectl get secret -n vertice vertice-core-secrets -o yaml
# Comparar com postgres-0 actual password
kubectl exec -n vertice postgres-0 -- env | grep POSTGRES_PASSWORD
# Se divergir: atualizar secret
```

### Fix Candidato 2: Validar Database
```bash
# Verificar se DB existe
kubectl exec -n vertice postgres-0 -- psql -U postgres -c "\l" | grep vertice
# Se n√£o existir: criar
kubectl exec -n vertice postgres-0 -- psql -U postgres -c "CREATE DATABASE vertice;"
```

### Fix Candidato 3: Validar User
```bash
# Verificar user
kubectl exec -n vertice postgres-0 -- psql -U postgres -c "\du" | grep vertice
# Se n√£o existir: criar
kubectl exec -n vertice postgres-0 -- psql -U postgres -c "CREATE USER vertice WITH PASSWORD 'XXX';"
kubectl exec -n vertice postgres-0 -- psql -U postgres -c "GRANT ALL PRIVILEGES ON DATABASE vertice TO vertice;"
```

### A√ß√£o de Fix
```bash
# Ap√≥s identificar causa-raiz:
# 1. Aplicar fix espec√≠fico
# 2. For√ßar restart do deployment
kubectl rollout restart deployment/hitl-patch-service -n vertice

# 3. Monitorar restart
kubectl rollout status deployment/hitl-patch-service -n vertice

# 4. Verificar logs
kubectl logs -n vertice -l app=hitl-patch-service --tail=50 -f
```

### Valida√ß√£o de Sucesso
```bash
# Pod deve estar Running
kubectl get pods -n vertice -l app=hitl-patch-service

# Service deve responder
kubectl exec -n vertice -l app=api-gateway -- curl -s http://hitl-patch-service:8027/health
```

### Rollback (se falhar)
```bash
# N√£o necess√°rio - apenas restart n√£o quebra nada
# Se secret foi alterado e causou problemas:
kubectl apply -f k8s/secrets/vertice-core-secrets.yaml.backup
kubectl rollout restart deployment/hitl-patch-service -n vertice
```

---

## GAP-2: command-bus-service (ALTO)

### Diagn√≥stico
```bash
# 1. Logs do pod
kubectl logs -n vertice -l app=command-bus-service --tail=100

# 2. Verificar deployment
kubectl describe deployment -n vertice command-bus-service

# 3. Verificar depend√™ncias (Redis, Kafka, etc)
kubectl get pods -n vertice -l app=redis
kubectl get pods -n vertice -l app=kafka
```

### Poss√≠veis Causas
1. Redis/Kafka connection failure
2. Missing environment variable
3. Port conflict
4. Resource limits too low

### A√ß√£o de Fix
```bash
# Aguardar an√°lise de logs para definir fix espec√≠fico
# N√ÉO fazer altera√ß√µes antes de diagn√≥stico completo
```

### Valida√ß√£o de Sucesso
```bash
kubectl get pods -n vertice -l app=command-bus-service
# Status: Running, Ready: 1/1
```

---

## GAP-3: agent-communication (ALTO)

### Diagn√≥stico
```bash
# 1. Logs do pod
kubectl logs -n vertice -l app=agent-communication --tail=100

# 2. Verificar deployment
kubectl describe deployment -n vertice agent-communication
```

### A√ß√£o de Fix
```bash
# Aguardar an√°lise de logs para definir fix espec√≠fico
```

### Valida√ß√£o de Sucesso
```bash
kubectl get pods -n vertice -l app=agent-communication
# Status: Running
```

---

## GAP-4: Demais Pods CrashLoop (11 pods)

### Prioriza√ß√£o

| Pod | Prioridade | Justificativa | Dashboard Afetado |
|-----|------------|---------------|-------------------|
| verdict-engine-service | ALTA | Cockpit Soberano cr√≠tico | Cockpit |
| hcl-kb-service | M√âDIA | HCL workflows afetados | N/A |
| threat-intel-bridge | M√âDIA | Intel aggregation degradada | N/A |
| memory-consolidation-service | BAIXA | Funcionalidade background | N/A |
| narrative-filter-service | BAIXA | Redund√¢ncia existe | Cockpit |
| offensive-orchestrator-service | BAIXA | N√£o usado em prod | Offensive |
| offensive-tools-service (2x) | BAIXA | Redund√¢ncia | Offensive |
| purple-team | BAIXA | Apenas em wargames | Purple Team |
| tegumentar-service (2x) | BAIXA | Sensory layer opcional | N/A |
| vertice-register | M√âDIA | Service registry backup | N/A |

### Estrat√©gia
1. **ALTA**: Fix ap√≥s GAP-1/2/3
2. **M√âDIA**: Fix apenas se tempo permitir
3. **BAIXA**: Documentar, n√£o fix (n√£o impactam dashboards principais)

### Metodologia por Pod
```bash
# Para cada pod:
# 1. kubectl logs -n vertice -l app=<pod-name> --tail=100
# 2. Identificar causa-raiz
# 3. Se fix simples (secret, env var): aplicar
# 4. Se fix complexo (c√≥digo, arquitetura): documentar como KNOWN ISSUE
```

---

## üö´ O QUE N√ÉO FAZER

### ‚ùå PROIBIDO
- Adicionar novos services/deployments
- Modificar c√≥digo de aplica√ß√£o (exceto config/env vars)
- Aumentar resources (CPU/Memory) sem justificativa t√©cnica
- Alterar arquitetura de rede (LoadBalancer, Ingress)
- Tocar em pods que est√£o Running (api-gateway, maximus-core, etc)
- Fazer "melhorias" ou "otimiza√ß√µes"
- Adicionar monitoring/logging novo

### ‚úÖ PERMITIDO
- Corrigir secrets corrompidos
- Ajustar env vars incorretas
- Criar databases/users missing
- Restart de deployments
- Ajustar probes (liveness/readiness) se timeout incorreto
- Corrigir typos em configs

---

## üìä M√âTRICAS DE SUCESSO

### Targets

| M√©trica | Atual | Target | Cr√≠tico |
|---------|-------|--------|---------|
| Pods Running | 85/99 (85.9%) | 92/99 (93%) | 90/99 (91%) |
| Dashboards Funcionais | 5/7 (71%) | 7/7 (100%) | 6/7 (86%) |
| CrashLoop Pods | 14 | 7 | 8 |
| Health Score | 87% | 93% | 91% |

### Defini√ß√£o de Sucesso M√≠nimo (Cr√≠tico)
- ‚úÖ HITL Dashboard UP
- ‚úÖ Command-Bus operational
- ‚úÖ Agent-Communication operational
- ‚úÖ ‚â•6 dashboards 100% funcionais
- ‚úÖ ‚â•91% pods Running

### Defini√ß√£o de Sucesso Ideal (Target)
- ‚úÖ Todos os dashboards funcionais
- ‚úÖ ‚â§7 pods CrashLoop (apenas prioridade BAIXA)
- ‚úÖ ‚â•93% Health Score

---

## üîÑ PROCESSO DE EXECU√á√ÉO

### Fase 1: Prepara√ß√£o (15min)
```bash
# 1. Backup de secrets atuais
kubectl get secret -n vertice vertice-core-secrets -o yaml > /tmp/secrets-backup.yaml

# 2. Documentar estado atual
kubectl get pods -n vertice > /tmp/pods-before.txt

# 3. Validar acesso kubectl
kubectl cluster-info
```

### Fase 2: GAP-1 (30min)
```bash
# Executar plano GAP-1 completo
# Checkpoint: HITL Dashboard deve estar UP
```

### Fase 3: GAP-2 (30min)
```bash
# Executar plano GAP-2 completo
# Checkpoint: Command-Bus operational
```

### Fase 4: GAP-3 (30min)
```bash
# Executar plano GAP-3 completo
# Checkpoint: Agent-Communication operational
```

### Fase 5: Valida√ß√£o Final (15min)
```bash
# 1. Verificar m√©tricas de sucesso
kubectl get pods -n vertice --field-selector=status.phase=Running | wc -l

# 2. Testar dashboards manualmente
# - MAXIMUS ‚úÖ
# - OSINT ‚úÖ
# - HITL ‚úÖ (novo)
# - Admin ‚úÖ
# - Consciousness ‚úÖ

# 3. Documentar resultado
echo "Pods Running: $(kubectl get pods -n vertice --field-selector=status.phase=Running | wc -l)/99" > /tmp/result.txt
```

### Fase 6: Documenta√ß√£o (15min)
```bash
# Atualizar DIAGNOSTIC_SUMMARY.md com novos n√∫meros
# Criar MAINTENANCE_REPORT_2025-10-26.md
```

---

## ‚è±Ô∏è TIMELINE ESTIMADO

| Fase | Dura√ß√£o | Hor√°rio (BRT) |
|------|---------|---------------|
| Prepara√ß√£o | 15min | 07:30-07:45 |
| GAP-1 Fix | 30min | 07:45-08:15 |
| GAP-2 Fix | 30min | 08:15-08:45 |
| GAP-3 Fix | 30min | 08:45-09:15 |
| Valida√ß√£o | 15min | 09:15-09:30 |
| Documenta√ß√£o | 15min | 09:30-09:45 |
| **TOTAL** | **2h15min** | - |

---

## üìù CHECKLIST DE EXECU√á√ÉO

### Antes de Come√ßar
- [ ] Backup de secrets criado
- [ ] Estado atual documentado
- [ ] Kubectl acesso validado
- [ ] Arquiteto-Chefe dispon√≠vel para aprova√ß√µes

### Durante Execu√ß√£o
- [ ] GAP-1: Logs analisados
- [ ] GAP-1: Causa-raiz identificada
- [ ] GAP-1: Fix aplicado
- [ ] GAP-1: Valida√ß√£o OK
- [ ] GAP-2: Logs analisados
- [ ] GAP-2: Causa-raiz identificada
- [ ] GAP-2: Fix aplicado
- [ ] GAP-2: Valida√ß√£o OK
- [ ] GAP-3: Logs analisados
- [ ] GAP-3: Causa-raiz identificada
- [ ] GAP-3: Fix aplicado
- [ ] GAP-3: Valida√ß√£o OK

### Ap√≥s Execu√ß√£o
- [ ] M√©tricas de sucesso atingidas
- [ ] Todos os dashboards cr√≠ticos testados
- [ ] Documenta√ß√£o atualizada
- [ ] Relat√≥rio de manuten√ß√£o gerado

---

## üÜò PLANO DE CONTING√äNCIA

### Se GAP-1 n√£o resolver ap√≥s 30min
**A√á√ÉO**: Documentar como KNOWN ISSUE, prosseguir para GAP-2/3  
**RAZ√ÉO**: N√£o bloquear outros fixes por 1 pod

### Se algum fix quebrar pod funcional
**A√á√ÉO**: Rollback imediato
```bash
kubectl rollout undo deployment/<deployment-name> -n vertice
```

### Se Health Score piorar
**A√á√ÉO**: STOP, rollback de todas as mudan√ßas
```bash
kubectl apply -f /tmp/secrets-backup.yaml
kubectl rollout restart deployment -n vertice --all
```

---

## üìÑ ENTREG√ÅVEIS

1. **MAINTENANCE_REPORT_2025-10-26.md**
   - Gaps resolvidos
   - Gaps n√£o resolvidos (com justificativa)
   - M√©tricas antes/depois
   - Pr√≥ximos passos (se houver)

2. **Secrets/Configs corrigidos** (se aplic√°vel)
   - Backup do estado anterior
   - Diff das mudan√ßas

3. **Lista de KNOWN ISSUES** (se houver)
   - Pods que n√£o foram fixados
   - Raz√£o t√©cnica
   - Workaround (se existir)

---

## ‚úÖ CRIT√âRIO DE CONCLUS√ÉO

Manuten√ß√£o √© considerada **COMPLETA** quando:

1. **OBRIGAT√ìRIO**:
   - ‚úÖ GAP-1 resolvido OU documentado como imposs√≠vel com justificativa
   - ‚úÖ GAP-2 resolvido OU documentado
   - ‚úÖ GAP-3 resolvido OU documentado
   - ‚úÖ Health Score ‚â• 91%
   - ‚úÖ Nenhum pod funcional foi quebrado
   - ‚úÖ Relat√≥rio de manuten√ß√£o gerado

2. **DESEJ√ÅVEL** (n√£o bloqueante):
   - 7/7 dashboards funcionais
   - ‚â§7 pods CrashLoop
   - Health Score ‚â•93%

---

**Aprova√ß√£o necess√°ria do Arquiteto-Chefe antes de iniciar Fase 2 (execu√ß√£o)**

---

**Vers√£o**: 1.0  
**Status**: AGUARDANDO APROVA√á√ÉO  
**Pr√≥ximo passo**: Aprova√ß√£o do Arquiteto-Chefe ‚Üí In√≠cio da Fase 1
