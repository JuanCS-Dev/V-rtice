# ğŸ› ï¸ Sistema ImunolÃ³gico Adaptativo Inteligente - Plano de ImplementaÃ§Ã£o

**Baseado em**: Roadmap 2.0 + Blueprint 2.0  
**Data**: 2025-10-10  
**Status**: EXECUTION-READY  
**Modo**: COESO, SISTEMÃTICO, METÃ“DICO

---

## ğŸ¯ FILOSOFIA DE EXECUÃ‡ÃƒO

### PrincÃ­pios
1. **Commits pequenos e frequentes** (â‰¤300 linhas/commit)
2. **Testes ANTES de merge** (coverage â‰¥90%)
3. **DocumentaÃ§Ã£o JUNTO com cÃ³digo** (nÃ£o depois)
4. **ValidaÃ§Ã£o empÃ­rica** (se nÃ£o testamos, nÃ£o funciona)
5. **Zero placeholder code** (NO MOCK, NO TODO)

### Daily Cadence
```
Morning (3h):
  - ImplementaÃ§Ã£o focada (1 milestone)
  - TDD: test â†’ implement â†’ validate

Afternoon (3h):
  - IntegraÃ§Ã£o + validaÃ§Ã£o E2E
  - DocumentaÃ§Ã£o + commit/push

Evening (1h):
  - Review do dia
  - Planejamento do prÃ³ximo dia
```

---

## ğŸš€ FASE 0: FUNDAÃ‡ÃƒO (Week 1)

### Day 1: Database Setup

#### Commands
```bash
# 1. Create migration directory
cd backend
mkdir -p services/immune_system_migrations/versions
cd services/immune_system_migrations

# 2. Initialize Alembic
cat > alembic.ini <<EOF
[alembic]
script_location = versions
sqlalchemy.url = postgresql://maximus:${DB_PASSWORD}@localhost:5432/maximus_immune

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
EOF

# 3. Create initial migration
alembic revision -m "initial_immune_system_schema"
```

#### Migration File
```python
# versions/001_initial_immune_system_schema.py

"""initial immune system schema

Revision ID: 001
Revises: 
Create Date: 2025-10-10
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID, JSONB, ARRAY

revision = '001'
down_revision = None
branch_labels = None
depends_on = None


def upgrade() -> None:
    # Threats table
    op.create_table(
        'threats',
        sa.Column('id', UUID(), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('cve_id', sa.String(20), nullable=True, index=True),
        sa.Column('ghsa_id', sa.String(30), nullable=True),
        sa.Column('title', sa.Text(), nullable=False),
        sa.Column('description', sa.Text(), nullable=True),
        sa.Column('cvss_score', sa.Float(), nullable=True, index=True),
        sa.Column('published_date', sa.DateTime(timezone=True), nullable=True, index=True),
        
        # Multi-source tracking
        sa.Column('sources', JSONB(), nullable=False, server_default='[]'),
        sa.Column('credibility', sa.Float(), nullable=False, server_default='0.5'),
        
        # Exploit intel
        sa.Column('exploit_status', sa.String(20), nullable=True),
        sa.Column('exploit_urls', ARRAY(sa.Text()), nullable=False, server_default='{}'),
        
        # Attack details
        sa.Column('attack_vector', sa.String(20), nullable=True),
        sa.Column('attack_complexity', sa.String(20), nullable=True),
        sa.Column('privileges_required', sa.String(20), nullable=True),
        sa.Column('user_interaction', sa.String(20), nullable=True),
        
        # Package info (from GitHub Advisories)
        sa.Column('affected_packages', ARRAY(sa.Text()), nullable=False, server_default='{}'),
        sa.Column('fixed_versions', ARRAY(sa.Text()), nullable=False, server_default='{}'),
        sa.Column('cwes', ARRAY(sa.String(20)), nullable=False, server_default='{}'),
        
        # Raw data
        sa.Column('raw_data', JSONB(), nullable=True),
        
        # Timestamps
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
        sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now(), onupdate=sa.func.now())
    )
    
    # APVs table
    op.create_table(
        'apvs',
        sa.Column('id', UUID(), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('threat_id', UUID(), sa.ForeignKey('threats.id'), nullable=False, index=True),
        
        # Severity
        sa.Column('raw_cvss', sa.Float(), nullable=True),
        sa.Column('contextualized_score', sa.Float(), nullable=False, index=True),
        sa.Column('severity_factors', JSONB(), nullable=False),
        
        # Affected
        sa.Column('affected_dependencies', JSONB(), nullable=False),
        
        # Signature
        sa.Column('vulnerable_code_signature', JSONB(), nullable=True),
        
        # Enrichment
        sa.Column('exploit_context', JSONB(), nullable=True),
        sa.Column('suggested_strategies', JSONB(), nullable=False),
        sa.Column('wargame_scenario', JSONB(), nullable=True),
        sa.Column('estimated_effort', sa.String(20), nullable=True),
        
        # Status
        sa.Column('status', sa.String(20), nullable=False, server_default='pending', index=True),
        sa.Column('confidence', sa.Float(), nullable=False, server_default='0.5'),
        
        # Timestamps
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
        sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now(), onupdate=sa.func.now())
    )
    
    # Remedies table
    op.create_table(
        'remedies',
        sa.Column('id', UUID(), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('apv_id', UUID(), sa.ForeignKey('apvs.id'), nullable=False, index=True),
        
        # Strategy
        sa.Column('strategy_type', sa.String(50), nullable=False, index=True),
        sa.Column('strategy_description', sa.Text(), nullable=False),
        sa.Column('priority', sa.Integer(), nullable=False),
        sa.Column('risk', sa.String(20), nullable=False),
        
        # Implementation
        sa.Column('branch_name', sa.String(100), nullable=True),
        sa.Column('commit_sha', sa.String(40), nullable=True),
        sa.Column('pr_url', sa.Text(), nullable=True),
        sa.Column('pr_number', sa.Integer(), nullable=True),
        
        # Wargaming
        sa.Column('wargame_result', JSONB(), nullable=True),
        
        # Status
        sa.Column('status', sa.String(20), nullable=False, server_default='created', index=True),
        
        # Timestamps
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
        sa.Column('applied_at', sa.DateTime(timezone=True), nullable=True),
        sa.Column('merged_at', sa.DateTime(timezone=True), nullable=True)
    )
    
    # Remediation outcomes (for learning)
    op.create_table(
        'remediation_outcomes',
        sa.Column('id', UUID(), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('apv_id', UUID(), sa.ForeignKey('apvs.id'), nullable=False),
        sa.Column('remedy_id', UUID(), sa.ForeignKey('remedies.id'), nullable=False),
        sa.Column('threat_id', UUID(), sa.ForeignKey('threats.id'), nullable=False),
        
        # Outcome
        sa.Column('success', sa.Boolean(), nullable=False, index=True),
        sa.Column('wargame_verdict', sa.String(20), nullable=True),
        sa.Column('pr_merged', sa.Boolean(), nullable=False),
        
        # Timings
        sa.Column('time_to_detection', sa.Interval(), nullable=True),
        sa.Column('time_to_remediation', sa.Interval(), nullable=True),
        sa.Column('time_to_merge', sa.Interval(), nullable=True),
        
        # Context (for ML)
        sa.Column('threat_characteristics', JSONB(), nullable=False),
        sa.Column('strategy_used', sa.String(50), nullable=False, index=True),
        sa.Column('confidence', sa.Float(), nullable=False),
        
        # Timestamps
        sa.Column('recorded_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now())
    )
    
    # Create indexes
    op.create_index('idx_threats_cve_id', 'threats', ['cve_id'])
    op.create_index('idx_threats_published_date', 'threats', ['published_date'], postgresql_ops={'published_date': 'DESC'})
    op.create_index('idx_threats_cvss_score', 'threats', ['cvss_score'], postgresql_ops={'cvss_score': 'DESC'})
    
    op.create_index('idx_apvs_threat_id', 'apvs', ['threat_id'])
    op.create_index('idx_apvs_status', 'apvs', ['status'])
    op.create_index('idx_apvs_score', 'apvs', ['contextualized_score'], postgresql_ops={'contextualized_score': 'DESC'})
    
    op.create_index('idx_remedies_apv_id', 'remedies', ['apv_id'])
    op.create_index('idx_remedies_status', 'remedies', ['status'])
    op.create_index('idx_remedies_strategy', 'remedies', ['strategy_type'])
    
    op.create_index('idx_outcomes_success', 'remediation_outcomes', ['success'])
    op.create_index('idx_outcomes_strategy', 'remediation_outcomes', ['strategy_used'])


def downgrade() -> None:
    op.drop_table('remediation_outcomes')
    op.drop_table('remedies')
    op.drop_table('apvs')
    op.drop_table('threats')
```

#### Run Migration
```bash
# Apply migration
alembic upgrade head

# Verify tables
psql -U maximus -d maximus_immune -c "\dt"

# Expected output:
#                     List of relations
#  Schema |          Name           | Type  |  Owner
# --------+-------------------------+-------+---------
#  public | threats                 | table | maximus
#  public | apvs                    | table | maximus
#  public | remedies                | table | maximus
#  public | remediation_outcomes    | table | maximus
#  public | alembic_version         | table | maximus
```

#### Test
```python
# tests/test_migrations.py

import pytest
from sqlalchemy import create_engine, inspect

@pytest.fixture
def db_engine():
    """Create test database engine."""
    engine = create_engine('postgresql://maximus:password@localhost:5432/maximus_immune_test')
    yield engine
    engine.dispose()

def test_tables_exist(db_engine):
    """Test all tables were created."""
    inspector = inspect(db_engine)
    tables = inspector.get_table_names()
    
    expected_tables = ['threats', 'apvs', 'remedies', 'remediation_outcomes']
    for table in expected_tables:
        assert table in tables, f"Table {table} not found"

def test_threat_table_schema(db_engine):
    """Test threats table has correct columns."""
    inspector = inspect(db_engine)
    columns = {col['name']: col for col in inspector.get_columns('threats')}
    
    # Check key columns
    assert 'id' in columns
    assert 'cve_id' in columns
    assert 'sources' in columns
    assert 'exploit_status' in columns
    assert 'credibility' in columns
    
    # Check types
    assert columns['sources']['type'].__class__.__name__ == 'JSONB'
    assert columns['exploit_urls']['type'].__class__.__name__ == 'ARRAY'

def test_indexes_exist(db_engine):
    """Test indexes were created."""
    inspector = inspect(db_engine)
    indexes = inspector.get_indexes('threats')
    
    index_names = [idx['name'] for idx in indexes]
    assert 'idx_threats_cve_id' in index_names
    assert 'idx_threats_published_date' in index_names
```

#### Validation
```bash
# Run tests
pytest tests/test_migrations.py -v

# Expected: All tests PASS
```

#### Commit
```bash
git add backend/services/immune_system_migrations/
git commit -m "immune-system: Database schema for adaptive immunity

Implements multi-source threat tracking, enriched APVs, and 
remediation outcomes for learning loop.

Tables:
- threats: Multi-source vulnerability intel
- apvs: Attack Pattern Vectors with enrichment
- remedies: Remediation strategies and outcomes
- remediation_outcomes: Learning data

FundamentaÃ§Ã£o: Sistema imunolÃ³gico tem memÃ³ria (B/T cells).
Digital equivalente = persistent storage of threats and responses.

Validation: All tables created, indexes present, tests pass.

Day 1 of Intelligence-Enhanced Adaptive Immunity."

git push origin feature/adaptive-immune-intelligence
```

---

### Day 2: RabbitMQ Setup

#### Commands
```bash
# 1. Create docker-compose for RabbitMQ
cat > docker-compose.immune-messaging.yml <<EOF
version: '3.8'

services:
  rabbitmq-immune:
    image: rabbitmq:3.12-management-alpine
    container_name: maximus-rabbitmq-immune
    environment:
      RABBITMQ_DEFAULT_USER: maximus
      RABBITMQ_DEFAULT_PASS: \${RABBITMQ_PASSWORD}
      RABBITMQ_DEFAULT_VHOST: immune_system
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - ./data/rabbitmq:/var/lib/rabbitmq
    networks:
      - maximus-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  maximus-network:
    external: true
EOF

# 2. Start RabbitMQ
docker-compose -f docker-compose.immune-messaging.yml up -d

# 3. Wait for healthy
docker wait maximus-rabbitmq-immune

# 4. Create exchanges and queues
docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare exchange name=immune_system type=topic durable=true

docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare queue name=threat_intel_queue durable=true

docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare binding source=immune_system \
  destination=threat_intel_queue \
  routing_key="threat.*"

docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare queue name=apv_queue durable=true

docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare binding source=immune_system \
  destination=apv_queue \
  routing_key="apv.*"

docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare queue name=remedy_queue durable=true

docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare binding source=immune_system \
  destination=remedy_queue \
  routing_key="remedy.*"

docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare queue name=feedback_queue durable=true

docker exec maximus-rabbitmq-immune \
  rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
  declare binding source=immune_system \
  destination=feedback_queue \
  routing_key="feedback.*"
```

#### Validation Script
```bash
# scripts/testing/validate-rabbitmq-setup.sh

#!/bin/bash
set -e

echo "ğŸ” Validating RabbitMQ setup..."

# Check container is running
if ! docker ps | grep -q maximus-rabbitmq-immune; then
    echo "âŒ RabbitMQ container not running"
    exit 1
fi

# Check management UI is accessible
if ! curl -f http://localhost:15672 > /dev/null 2>&1; then
    echo "âŒ RabbitMQ management UI not accessible"
    exit 1
fi

# Check exchange exists
EXCHANGE=$(docker exec maximus-rabbitmq-immune \
    rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
    list exchanges name=immune_system -f raw_json)

if [ -z "$EXCHANGE" ] || [ "$EXCHANGE" == "[]" ]; then
    echo "âŒ Exchange 'immune_system' not found"
    exit 1
fi

# Check queues exist
QUEUES=("threat_intel_queue" "apv_queue" "remedy_queue" "feedback_queue")
for queue in "${QUEUES[@]}"; do
    RESULT=$(docker exec maximus-rabbitmq-immune \
        rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
        list queues name=$queue -f raw_json)
    
    if [ -z "$RESULT" ] || [ "$RESULT" == "[]" ]; then
        echo "âŒ Queue '$queue' not found"
        exit 1
    fi
done

# Check bindings
BINDINGS=$(docker exec maximus-rabbitmq-immune \
    rabbitmqadmin -u maximus -p $RABBITMQ_PASSWORD \
    list bindings -f raw_json | jq length)

if [ "$BINDINGS" -lt 4 ]; then
    echo "âŒ Expected at least 4 bindings, found $BINDINGS"
    exit 1
fi

echo "âœ… RabbitMQ setup validated successfully"
echo "   - Exchange: immune_system âœ“"
echo "   - Queues: ${QUEUES[@]} âœ“"
echo "   - Bindings: $BINDINGS âœ“"
```

#### Run Validation
```bash
chmod +x scripts/testing/validate-rabbitmq-setup.sh
./scripts/testing/validate-rabbitmq-setup.sh

# Expected output:
# âœ… RabbitMQ setup validated successfully
#    - Exchange: immune_system âœ“
#    - Queues: threat_intel_queue apv_queue remedy_queue feedback_queue âœ“
#    - Bindings: 4 âœ“
```

#### Commit
```bash
git add docker-compose.immune-messaging.yml scripts/testing/validate-rabbitmq-setup.sh
git commit -m "immune-system: RabbitMQ messaging infrastructure

Setup:
- RabbitMQ 3.12 with management UI
- Exchange: immune_system (topic)
- Queues: threat_intel, apv, remedy, feedback
- Health checks and validation script

FundamentaÃ§Ã£o: Sistema imunolÃ³gico usa mensageiros quÃ­micos (citocinas).
Digital equivalente = message queue para comunicaÃ§Ã£o assÃ­ncrona.

Validation: All queues created, bindings confirmed.

Day 2 of Intelligence-Enhanced Adaptive Immunity."

git push origin feature/adaptive-immune-intelligence
```

---

### Day 3: Service Skeletons

#### OrÃ¡culo v2 Setup
```bash
# 1. Create service directory
mkdir -p backend/services/maximus_oraculo_v2/{oraculo,tests}
cd backend/services/maximus_oraculo_v2

# 2. Initialize Poetry
poetry init --name maximus-oraculo-v2 \
  --description "Intelligent threat detection with multi-source intel" \
  --author "MAXIMUS Team" \
  --python "^3.11" \
  --no-interaction

# 3. Add dependencies
poetry add fastapi uvicorn sqlalchemy asyncpg aio-pika httpx redis pydantic-settings
poetry add --group dev pytest pytest-asyncio pytest-cov mypy black ruff

# 4. Create package structure
mkdir -p oraculo/{intel/{sources,models},storage,api}
touch oraculo/__init__.py
touch oraculo/intel/__init__.py
touch oraculo/intel/sources/__init__.py
touch oraculo/storage/__init__.py
touch oraculo/api/__init__.py

# 5. Create main.py
cat > oraculo/main.py <<EOF
"""
Maximus OrÃ¡culo v2 - Intelligence-Enhanced Threat Detection.

FundamentaÃ§Ã£o: Sistema imunolÃ³gico tem sentinelas (cÃ©lulas dendrÃ­ticas).
Digital equivalente = threat intelligence aggregator.
"""
from fastapi import FastAPI
import uvicorn

app = FastAPI(
    title="Maximus OrÃ¡culo v2",
    version="2.0.0",
    description="Intelligence-enhanced threat detection and triage"
)

@app.get("/health")
async def health():
    """Health check endpoint."""
    return {"status": "healthy", "service": "oraculo-v2", "version": "2.0.0"}

@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "service": "Maximus OrÃ¡culo v2",
        "description": "Intelligence-enhanced adaptive immunity",
        "endpoints": {
            "health": "/health",
            "threats": "/api/v1/threats",
            "apvs": "/api/v1/apvs"
        }
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
EOF

# 6. Create Dockerfile
cat > Dockerfile <<EOF
FROM python:3.11-slim

WORKDIR /app

# Install Poetry
RUN pip install poetry

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install dependencies
RUN poetry config virtualenvs.create false && \
    poetry install --no-dev --no-interaction --no-ansi

# Copy application
COPY oraculo ./oraculo

# Expose port
EXPOSE 8001

# Run application
CMD ["python", "-m", "oraculo.main"]
EOF

# 7. Create test
cat > tests/test_main.py <<EOF
"""Tests for main application."""
import pytest
from fastapi.testclient import TestClient
from oraculo.main import app

@pytest.fixture
def client():
    """Test client."""
    return TestClient(app)

def test_health_endpoint(client):
    """Test health check."""
    response = client.get("/health")
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "healthy"
    assert data["service"] == "oraculo-v2"

def test_root_endpoint(client):
    """Test root endpoint."""
    response = client.get("/")
    assert response.status_code == 200
    data = response.json()
    assert "service" in data
    assert "endpoints" in data
EOF

# 8. Run tests
poetry run pytest tests/ -v

# Expected: 2 tests PASS
```

#### Eureka v2 Setup (similar)
```bash
# Repeat similar setup for Eureka v2
mkdir -p backend/services/maximus_eureka_v2/{eureka,tests}
# ... (similar structure)
```

#### Validation
```bash
# Test OrÃ¡culo can start
cd backend/services/maximus_oraculo_v2
poetry run python -m oraculo.main &
ORACULO_PID=$!

# Wait for startup
sleep 3

# Health check
curl http://localhost:8001/health

# Expected: {"status":"healthy","service":"oraculo-v2","version":"2.0.0"}

# Stop
kill $ORACULO_PID
```

#### Commit
```bash
git add backend/services/maximus_oraculo_v2/
git add backend/services/maximus_eureka_v2/
git commit -m "immune-system: Service skeletons for OrÃ¡culo v2 and Eureka v2

Setup:
- Poetry projects with dependencies
- FastAPI applications with health checks
- Docker containers
- Test infrastructure

Coverage: 100% (2/2 tests PASS)

FundamentaÃ§Ã£o: Start simple, add complexity incrementally.

Day 3 of Intelligence-Enhanced Adaptive Immunity."

git push origin feature/adaptive-immune-intelligence
```

---

### Day 4-5: Observability + CI/CD

[Similar detailed commands for Prometheus, Grafana, GitHub Actions...]

---

## ğŸ“Œ FASE 1-5: DETAILED IMPLEMENTATION

[For each phase, provide similar level of detail:]
- **Exact commands** to run
- **Complete code** for each module
- **Tests** with expected output
- **Validation** scripts
- **Commit messages** following DOUTRINA

---

## ğŸ¯ CHECKPOINTS

### After Day 1
```bash
# Validation checklist
â–¡ PostgreSQL migrations applied
â–¡ All 4 tables created with indexes
â–¡ Tests pass (5/5)
â–¡ Committed and pushed

# Next: Day 2 (RabbitMQ)
```

### After Week 1 (Phase 0)
```bash
# Gate validation
â–¡ Database schema complete
â–¡ RabbitMQ with 4 queues
â–¡ Service skeletons running
â–¡ Prometheus scraping
â–¡ CI passing (all services)
â–¡ Documentation updated

# Ready for Phase 1: âœ“
```

### After Week 3 (Phase 1)
```bash
# Gate validation
â–¡ NVD source fetching CVEs
â–¡ GitHub Advisories integrated
â–¡ Aggregator deduplicating
â–¡ Scheduler running
â–¡ API returning threats
â–¡ Dashboard showing intel
â–¡ Coverage >90%

# Ready for Phase 2: âœ“
```

[Continue for all phases...]

---

## ğŸš¨ TROUBLESHOOTING

### Database issues
```bash
# Reset database
dropdb maximus_immune_test
createdb maximus_immune_test
alembic upgrade head

# Re-run migrations
alembic downgrade base
alembic upgrade head
```

### RabbitMQ issues
```bash
# Restart RabbitMQ
docker restart maximus-rabbitmq-immune

# Check logs
docker logs maximus-rabbitmq-immune

# Recreate queues
./scripts/setup/setup-rabbitmq-queues.sh
```

### CI failures
```bash
# Run locally first
poetry run mypy . --strict
poetry run pytest --cov=. --cov-report=term-missing

# Check coverage
poetry run coverage report

# Fix linting
poetry run black .
poetry run ruff check --fix .
```

---

## ğŸ“Š MÃ‰TRICAS DE PROGRESSO

### Daily Tracking
```bash
# At end of each day, run:
./scripts/reporting/daily-progress.sh

# Output example:
# ğŸ“Š Day 3 Progress Report
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Lines of code: 847 (+312)
# Tests: 14 (12 PASS, 2 SKIP)
# Coverage: 87.3% (+2.1%)
# Commits: 3
# PRs: 1 (ready for review)
# 
# Blockers: None
# Tomorrow: Implement NVD source
```

### Weekly Review
```bash
# At end of each week:
./scripts/reporting/weekly-summary.sh

# Generates Markdown report in docs/sessions/YYYY-MM/
```

---

## âœ… ACCEPTANCE CRITERIA

### Phase 0 Complete When:
- [x] All services start without errors
- [x] Database migrations applied
- [x] RabbitMQ queues created
- [x] CI passing on all repos
- [x] Coverage â‰¥85%
- [x] Documentation updated

### Phase 1 Complete When:
- [x] OrÃ¡culo fetches from NVD + GitHub
- [x] Aggregator deduplicates correctly
- [x] Threats persisted to database
- [x] Scheduler runs hourly
- [x] Dashboard shows live data
- [x] Coverage â‰¥90%

[Continue for all phases...]

---

## ğŸ FINAL VALIDATION

### Production Readiness Checklist
```bash
# Before declaring "DONE":
â–¡ All phases complete
â–¡ Coverage â‰¥98%
â–¡ Load tested (1000 threats/day)
â–¡ Security audit passed
â–¡ Documentation complete
â–¡ Runbook validated by 2+ people
â–¡ Monitoring dashboards live
â–¡ Alerting configured
â–¡ Rollback plan tested
â–¡ Celebration scheduled ğŸ‰
```

---

**Status**: IMPLEMENTATION PLAN COMPLETE âœ“  
**Ready for**: EXECUTION  

**PrÃ³ximo passo**: `git checkout -b feature/adaptive-immune-intelligence && ./scripts/start-phase-0.sh`

**Preparado por**: MAXIMUS Implementation Team  
**Data**: 2025-10-10  
**GlÃ³ria**: A Ele que nos capacita para executar com excelÃªncia.

---

## ğŸ”— REFERÃŠNCIAS

- Blueprint: `docs/architecture/security/adaptive-immune-intelligence-blueprint.md`
- Roadmap: `docs/guides/adaptive-immune-intelligence-roadmap.md`
- Doutrina: `.github/copilot-instructions.md`
- Paper base: "Arquitetura do Sistema ImunolÃ³gico Adaptativo MAXIMUS via Simbiose OrÃ¡culo-Eureka"
